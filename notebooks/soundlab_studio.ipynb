{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83d\udd04 Resume From Checkpoint\n",
    "# @markdown Resume a previous run or start fresh. Set `resume_run_id` to reload\n",
    "# @markdown artifacts and skip completed stages.\n",
    "\n",
    "resume_run_id = \"\"  # @param {type:\"string\"}\n",
    "cache_root = \"/content/drive/MyDrive/soundlab_cache\"  # @param {type:\"string\"}\n",
    "enable_drive_cache = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# === Execution ===\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive if caching enabled\n",
    "if enable_drive_cache:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "\n",
    "        drive.mount(\"/content/drive\", force_remount=False)\n",
    "        print(\"\u2705 Google Drive mounted\")\n",
    "    except ImportError:\n",
    "        print(\"\u26a0\ufe0f Not running in Colab, skipping Drive mount\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Drive mount failed: {e}\")\n",
    "\n",
    "# Setup cache directories\n",
    "CACHE_ROOT = Path(cache_root)\n",
    "CACHE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUNS_DIR = CACHE_ROOT / \"runs\"\n",
    "CHECKPOINTS_DIR = CACHE_ROOT / \"checkpoints\"\n",
    "MODELS_DIR = CACHE_ROOT / \"models\"\n",
    "\n",
    "for d in [RUNS_DIR, CHECKPOINTS_DIR, MODELS_DIR]:\n",
    "    d.mkdir(exist_ok=True)\n",
    "\n",
    "# Resume logic\n",
    "RUN_ID: str | None = None\n",
    "RESUMED = False\n",
    "\n",
    "if resume_run_id.strip():\n",
    "    run_dir = RUNS_DIR / resume_run_id.strip()\n",
    "    if run_dir.exists():\n",
    "        RUN_ID = resume_run_id.strip()\n",
    "        RESUMED = True\n",
    "        print(f\"\u2705 Resuming run: {RUN_ID}\")\n",
    "        print(f\"   Run directory: {run_dir}\")\n",
    "    else:\n",
    "        print(f\"\u26a0\ufe0f Run '{resume_run_id}' not found, starting fresh\")\n",
    "else:\n",
    "    print(\"\u2139\ufe0f No resume ID provided, will generate new run ID on upload\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc1 Cache root: {CACHE_ROOT}\")\n",
    "print(f\"   Runs: {RUNS_DIR}\")\n",
    "print(f\"   Checkpoints: {CHECKPOINTS_DIR}\")\n",
    "print(f\"   Models: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udf9b\ufe0f SoundLab Studio\n",
    "\n",
    "**Production-ready music processing pipeline**\n",
    "\n",
    "---\n",
    "\n",
    "## Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| \ud83c\udf9a\ufe0f **Stem Separation** | Demucs HTDemucs/HTDemucs-FT models for vocals, drums, bass, other |\n",
    "| \ud83c\udfb9 **Audio-to-MIDI** | Basic Pitch transcription with onset/frame thresholds |\n",
    "| \ud83c\udfa8 **Effects Processing** | Pedalboard-based EQ, compression, reverb, and creative effects |\n",
    "| \ud83d\udcca **Audio Analysis** | Tempo, key, loudness (LUFS), spectral features |\n",
    "| \ud83d\udde3\ufe0f **Voice Generation** | XTTS-v2 TTS and RVC voice conversion (optional) |\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Setup** \u2014 Configure environment and install packages\n",
    "2. **Upload** \u2014 Load audio file and compute metadata\n",
    "3. **Separate** \u2014 Extract stems with quality-aware candidate selection\n",
    "4. **Transcribe** \u2014 Convert stems to MIDI with cleanup\n",
    "5. **Preview & QA** \u2014 Review results and re-run if needed\n",
    "6. **Export** \u2014 Download stems, MIDI, and reports\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- **GPU Runtime** recommended (T4 or better)\n",
    "- ~8GB VRAM for Demucs separation\n",
    "- ~4GB disk space for models\n",
    "\n",
    "---\n",
    "\n",
    "> \ud83d\udca1 **Tip:** Enable Google Drive caching in Cell 0 to persist models and resume runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \u2699\ufe0f Environment Setup\n",
    "# @markdown Configure the processing environment and runtime settings.\n",
    "\n",
    "gpu_mode = \"auto\"  # @param [\"auto\", \"force_gpu\", \"force_cpu\"]\n",
    "log_level = \"INFO\"  # @param [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"]\n",
    "output_base = \"/content/soundlab_outputs\"  # @param {type:\"string\"}\n",
    "deterministic = False  # @param {type:\"boolean\"}\n",
    "random_seed = 42  # @param {type:\"integer\"}\n",
    "\n",
    "# === Execution ===\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure environment variables\n",
    "os.environ[\"SOUNDLAB_LOG_LEVEL\"] = log_level\n",
    "os.environ[\"SOUNDLAB_GPU_MODE\"] = gpu_mode\n",
    "\n",
    "if deterministic:\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    os.environ[\"SOUNDLAB_SEED\"] = str(random_seed)\n",
    "\n",
    "# Setup output directories\n",
    "OUTPUT_DIR = Path(output_base)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STEMS_DIR = OUTPUT_DIR / \"stems\"\n",
    "MIDI_DIR = OUTPUT_DIR / \"midi\"\n",
    "EFFECTS_DIR = OUTPUT_DIR / \"effects\"\n",
    "ANALYSIS_DIR = OUTPUT_DIR / \"analysis\"\n",
    "VOICE_DIR = OUTPUT_DIR / \"voice\"\n",
    "EXPORTS_DIR = OUTPUT_DIR / \"exports\"\n",
    "\n",
    "for d in [STEMS_DIR, MIDI_DIR, EFFECTS_DIR, ANALYSIS_DIR, VOICE_DIR, EXPORTS_DIR]:\n",
    "    d.mkdir(exist_ok=True)\n",
    "\n",
    "# Runtime introspection\n",
    "import torch\n",
    "\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_NAME = torch.cuda.get_device_name(0) if GPU_AVAILABLE else \"N/A\"\n",
    "GPU_MEMORY = (\n",
    "    f\"{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\" if GPU_AVAILABLE else \"N/A\"\n",
    ")\n",
    "\n",
    "print(\"\ud83d\udda5\ufe0f Runtime Information\")\n",
    "print(f\"   GPU Available: {GPU_AVAILABLE}\")\n",
    "print(f\"   GPU Name: {GPU_NAME}\")\n",
    "print(f\"   GPU Memory: {GPU_MEMORY}\")\n",
    "print(f\"   GPU Mode: {gpu_mode}\")\n",
    "print(f\"   Deterministic: {deterministic}\")\n",
    "if deterministic:\n",
    "    print(f\"   Random Seed: {random_seed}\")\n",
    "print(f\"\\n\u2705 Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83d\udce6 Install SoundLab\n",
    "# @markdown Install the soundlab package and dependencies.\n",
    "\n",
    "install_voice = False  # @param {type:\"boolean\"}\n",
    "install_from = \"pypi\"  # @param [\"pypi\", \"github_main\", \"github_dev\"]\n",
    "force_reinstall = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# === Execution ===\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_package() -> None:\n",
    "    \"\"\"Install soundlab with appropriate extras.\"\"\"\n",
    "    extras = \"[notebook\"\n",
    "    if install_voice:\n",
    "        extras += \",voice\"\n",
    "    extras += \"]\"\n",
    "\n",
    "    if install_from == \"pypi\":\n",
    "        pkg = f\"soundlab{extras}\"\n",
    "    elif install_from == \"github_main\":\n",
    "        pkg = f\"soundlab{extras} @ git+https://github.com/wyattowalsh/soundlab.git\"\n",
    "    else:  # github_dev\n",
    "        pkg = f\"soundlab{extras} @ git+https://github.com/wyattowalsh/soundlab.git@dev\"\n",
    "\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg]\n",
    "    if force_reinstall:\n",
    "        cmd.append(\"--force-reinstall\")\n",
    "\n",
    "    print(f\"\ud83d\udce6 Installing: {pkg}\")\n",
    "    subprocess.check_call(cmd)\n",
    "    print(f\"\u2705 Installed: {pkg}\")\n",
    "\n",
    "\n",
    "install_package()\n",
    "\n",
    "# Verify installation\n",
    "import soundlab\n",
    "\n",
    "print(f\"\\n\u2705 SoundLab version: {soundlab.__version__}\")\n",
    "\n",
    "# Configure logging\n",
    "from soundlab.utils.logging import configure_logging\n",
    "\n",
    "configure_logging(level=log_level)\n",
    "\n",
    "# Show available features\n",
    "print(\"\\n\ud83d\udccb Available modules:\")\n",
    "print(\"   \u2705 soundlab.separation\")\n",
    "print(\"   \u2705 soundlab.transcription\")\n",
    "print(\"   \u2705 soundlab.effects\")\n",
    "print(\"   \u2705 soundlab.analysis\")\n",
    "print(\"   \u2705 soundlab.pipeline\")\n",
    "\n",
    "try:\n",
    "    from soundlab import voice\n",
    "\n",
    "    print(\"   \u2705 soundlab.voice (TTS + RVC)\")\n",
    "except ImportError:\n",
    "    print(\"   \u26aa soundlab.voice (not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83c\udfb5 Upload Audio\n",
    "# @markdown Upload your audio file for processing. Supported formats: WAV, MP3, FLAC, OGG, AIFF, M4A\n",
    "\n",
    "# === Execution ===\n",
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "from soundlab.io import load_audio\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from soundlab.core.audio import AudioSegment\n",
    "\n",
    "# Global state for current audio\n",
    "CURRENT_AUDIO: AudioSegment | None = None\n",
    "AUDIO_HASH: str | None = None\n",
    "SOURCE_PATH: Path | None = None\n",
    "\n",
    "\n",
    "def compute_audio_hash(path: Path) -> str:\n",
    "    \"\"\"Compute SHA-256 hash of audio file for caching/dedup.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()[:16]\n",
    "\n",
    "\n",
    "def handle_upload(audio_path: str | None) -> tuple[str, str]:\n",
    "    \"\"\"Handle audio file upload and display metadata.\"\"\"\n",
    "    global CURRENT_AUDIO, AUDIO_HASH, SOURCE_PATH\n",
    "\n",
    "    if audio_path is None:\n",
    "        CURRENT_AUDIO = None\n",
    "        AUDIO_HASH = None\n",
    "        SOURCE_PATH = None\n",
    "        return \"No file uploaded\", \"\"\n",
    "\n",
    "    try:\n",
    "        source = Path(audio_path)\n",
    "        CURRENT_AUDIO = load_audio(source)\n",
    "        AUDIO_HASH = compute_audio_hash(source)\n",
    "        SOURCE_PATH = source\n",
    "\n",
    "        meta = CURRENT_AUDIO.metadata\n",
    "        if meta is None:\n",
    "            info = f\"\"\"\n",
    "**File:** {source.name}\n",
    "**Duration:** {CURRENT_AUDIO.duration_seconds:.2f}s\n",
    "**Sample Rate:** {CURRENT_AUDIO.sample_rate} Hz\n",
    "**Channels:** {CURRENT_AUDIO.channels}\n",
    "**Hash:** `{AUDIO_HASH}`\n",
    "            \"\"\"\n",
    "        else:\n",
    "            info = f\"\"\"\n",
    "**File:** {source.name}\n",
    "**Duration:** {meta.duration_str}\n",
    "**Sample Rate:** {meta.sample_rate} Hz\n",
    "**Channels:** {meta.channels} ({\"Stereo\" if meta.is_stereo else \"Mono\"})\n",
    "**Bit Depth:** {meta.bit_depth.value if meta.bit_depth else \"Unknown\"}-bit\n",
    "**Format:** {meta.format.value.upper() if meta.format else \"Unknown\"}\n",
    "**Hash:** `{AUDIO_HASH}`\n",
    "            \"\"\"\n",
    "        return info.strip(), \"\u2705 Audio loaded successfully!\"\n",
    "\n",
    "    except Exception as e:\n",
    "        CURRENT_AUDIO = None\n",
    "        AUDIO_HASH = None\n",
    "        SOURCE_PATH = None\n",
    "        return f\"**Error:** {e}\", \"\u274c Failed to load audio\"\n",
    "\n",
    "\n",
    "# Build Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as upload_interface:\n",
    "    gr.Markdown(\"## \ud83c\udfb5 Upload Audio File\")\n",
    "    gr.Markdown(\"Supported formats: WAV, MP3, FLAC, OGG, AIFF, M4A\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            audio_input = gr.Audio(\n",
    "                label=\"Input Audio\",\n",
    "                type=\"filepath\",\n",
    "                sources=[\"upload\"],\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            info_output = gr.Markdown(label=\"Audio Info\", value=\"No file uploaded\")\n",
    "            status_output = gr.Markdown()\n",
    "\n",
    "    audio_input.change(\n",
    "        fn=handle_upload,\n",
    "        inputs=[audio_input],\n",
    "        outputs=[info_output, status_output],\n",
    "    )\n",
    "\n",
    "upload_interface.launch(height=450, show_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83d\udd0a Canonical Decode\n",
    "# @markdown Decode audio to canonical format (44.1kHz stereo float32) for consistent processing.\n",
    "\n",
    "target_sample_rate = 44100  # @param {type:\"integer\"}\n",
    "target_channels = 2  # @param [1, 2] {type:\"raw\"}\n",
    "normalize_audio = True  # @param {type:\"boolean\"}\n",
    "normalization_level = -1.0  # @param {type:\"number\"}\n",
    "\n",
    "# === Execution ===\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "def to_canonical(\n",
    "    audio: AudioSegment,\n",
    "    sr: int = 44100,\n",
    "    channels: int = 2,\n",
    "    normalize: bool = True,\n",
    "    level_db: float = -1.0,\n",
    ") -> AudioSegment:\n",
    "    \"\"\"\n",
    "    Convert audio to canonical format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio\n",
    "        Source AudioSegment.\n",
    "    sr\n",
    "        Target sample rate (default 44100).\n",
    "    channels\n",
    "        Target channel count (1=mono, 2=stereo).\n",
    "    normalize\n",
    "        Whether to normalize audio level.\n",
    "    level_db\n",
    "        Target peak level in dB (default -1.0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    AudioSegment\n",
    "        Resampled and normalized audio.\n",
    "    \"\"\"\n",
    "    from soundlab.core.audio import AudioSegment\n",
    "\n",
    "    samples = audio.samples.copy()\n",
    "    current_sr = audio.sample_rate\n",
    "\n",
    "    # Resample if needed\n",
    "    if current_sr != sr:\n",
    "        if samples.ndim == 1:\n",
    "            samples = signal.resample(samples, int(len(samples) * sr / current_sr))\n",
    "        else:\n",
    "            # Resample each channel\n",
    "            new_length = int(samples.shape[-1] * sr / current_sr)\n",
    "            resampled = np.zeros((samples.shape[0], new_length), dtype=np.float32)\n",
    "            for i in range(samples.shape[0]):\n",
    "                resampled[i] = signal.resample(samples[i], new_length)\n",
    "            samples = resampled\n",
    "        print(f\"   Resampled: {current_sr} Hz \u2192 {sr} Hz\")\n",
    "\n",
    "    # Convert channels\n",
    "    current_channels = 1 if samples.ndim == 1 else samples.shape[0]\n",
    "\n",
    "    if current_channels != channels:\n",
    "        if channels == 1 and current_channels == 2:\n",
    "            # Stereo to mono\n",
    "            samples = np.mean(samples, axis=0)\n",
    "            print(\"   Converted: Stereo \u2192 Mono\")\n",
    "        elif channels == 2 and current_channels == 1:\n",
    "            # Mono to stereo (duplicate)\n",
    "            if samples.ndim == 1:\n",
    "                samples = np.stack([samples, samples])\n",
    "            else:\n",
    "                samples = np.stack([samples[0], samples[0]])\n",
    "            print(\"   Converted: Mono \u2192 Stereo\")\n",
    "\n",
    "    # Normalize\n",
    "    if normalize:\n",
    "        peak = np.abs(samples).max()\n",
    "        if peak > 0:\n",
    "            target_peak = 10 ** (level_db / 20.0)\n",
    "            samples = samples * (target_peak / peak)\n",
    "            print(f\"   Normalized to {level_db} dB peak\")\n",
    "\n",
    "    return AudioSegment(\n",
    "        samples=samples.astype(np.float32),\n",
    "        sample_rate=sr,\n",
    "        source_path=audio.source_path,\n",
    "        metadata=audio.metadata,\n",
    "    )\n",
    "\n",
    "\n",
    "# Process current audio\n",
    "if CURRENT_AUDIO is None:\n",
    "    print(\"\u26a0\ufe0f Please upload an audio file first (Cell 4)\")\n",
    "else:\n",
    "    print(f\"\ud83d\udd0a Processing: {SOURCE_PATH.name if SOURCE_PATH else 'audio'}\")\n",
    "    print(f\"   Original: {CURRENT_AUDIO.sample_rate} Hz, {CURRENT_AUDIO.channels} channels\")\n",
    "\n",
    "    CANONICAL_AUDIO = to_canonical(\n",
    "        CURRENT_AUDIO,\n",
    "        sr=target_sample_rate,\n",
    "        channels=target_channels,\n",
    "        normalize=normalize_audio,\n",
    "        level_db=normalization_level,\n",
    "    )\n",
    "\n",
    "    print(f\"   Canonical: {CANONICAL_AUDIO.sample_rate} Hz, {CANONICAL_AUDIO.channels} channels\")\n",
    "    print(f\"   Duration: {CANONICAL_AUDIO.duration_seconds:.2f}s\")\n",
    "    print(f\"   Samples: {CANONICAL_AUDIO.samples.shape}\")\n",
    "    print(\"\\n\u2705 Audio decoded to canonical format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \u2702\ufe0f Excerpt Selection (Optional)\n",
    "# @markdown Extract a shorter excerpt for faster processing and candidate comparison.\n",
    "# @markdown Set `use_excerpt=False` to process the full track.\n",
    "\n",
    "use_excerpt = True  # @param {type:\"boolean\"}\n",
    "excerpt_start_seconds = 30.0  # @param {type:\"number\"}\n",
    "excerpt_duration_seconds = 30.0  # @param {type:\"number\"}\n",
    "auto_select_excerpt = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# === Execution ===\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "def extract_excerpt(\n",
    "    audio: AudioSegment,\n",
    "    start: float,\n",
    "    duration: float,\n",
    ") -> AudioSegment:\n",
    "    \"\"\"\n",
    "    Extract an excerpt from audio.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio\n",
    "        Source AudioSegment.\n",
    "    start\n",
    "        Start time in seconds.\n",
    "    duration\n",
    "        Duration in seconds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    AudioSegment\n",
    "        Extracted excerpt.\n",
    "    \"\"\"\n",
    "    from soundlab.core.audio import AudioSegment\n",
    "\n",
    "    sr = audio.sample_rate\n",
    "    samples = audio.samples\n",
    "\n",
    "    start_sample = int(start * sr)\n",
    "    end_sample = int((start + duration) * sr)\n",
    "\n",
    "    # Clamp to valid range\n",
    "    total_samples = samples.shape[-1] if samples.ndim > 1 else len(samples)\n",
    "    start_sample = max(0, min(start_sample, total_samples))\n",
    "    end_sample = max(start_sample, min(end_sample, total_samples))\n",
    "\n",
    "    if samples.ndim == 1:\n",
    "        excerpt_samples = samples[start_sample:end_sample]\n",
    "    else:\n",
    "        excerpt_samples = samples[:, start_sample:end_sample]\n",
    "\n",
    "    return AudioSegment(\n",
    "        samples=excerpt_samples,\n",
    "        sample_rate=sr,\n",
    "        source_path=audio.source_path,\n",
    "        metadata=audio.metadata,\n",
    "    )\n",
    "\n",
    "\n",
    "def find_energetic_region(audio: AudioSegment, duration: float) -> float:\n",
    "    \"\"\"\n",
    "    Find the most energetic region of the audio for excerpt selection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio\n",
    "        Source AudioSegment.\n",
    "    duration\n",
    "        Desired excerpt duration in seconds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Optimal start time in seconds.\n",
    "    \"\"\"\n",
    "    sr = audio.sample_rate\n",
    "    samples = audio.samples\n",
    "\n",
    "    # Convert to mono for analysis\n",
    "    mono = np.mean(samples, axis=0) if samples.ndim > 1 else samples\n",
    "\n",
    "    # Compute RMS energy in windows\n",
    "    window_samples = int(duration * sr)\n",
    "    hop_samples = int(sr)  # 1-second hop\n",
    "\n",
    "    if len(mono) <= window_samples:\n",
    "        return 0.0\n",
    "\n",
    "    best_energy = 0.0\n",
    "    best_start = 0.0\n",
    "\n",
    "    for start_idx in range(0, len(mono) - window_samples, hop_samples):\n",
    "        window = mono[start_idx : start_idx + window_samples]\n",
    "        energy = np.sqrt(np.mean(window**2))\n",
    "\n",
    "        if energy > best_energy:\n",
    "            best_energy = energy\n",
    "            best_start = start_idx / sr\n",
    "\n",
    "    return best_start\n",
    "\n",
    "\n",
    "# Process excerpt\n",
    "if \"CANONICAL_AUDIO\" not in dir() or CANONICAL_AUDIO is None:\n",
    "    print(\"\u26a0\ufe0f Please run the Canonical Decode cell first (Cell 5)\")\n",
    "else:\n",
    "    total_duration = CANONICAL_AUDIO.duration_seconds\n",
    "    print(f\"\ud83d\udcca Total duration: {total_duration:.2f}s\")\n",
    "\n",
    "    if not use_excerpt:\n",
    "        EXCERPT_AUDIO = CANONICAL_AUDIO\n",
    "        EXCERPT_START = 0.0\n",
    "        EXCERPT_DURATION = total_duration\n",
    "        print(\"\u2139\ufe0f Using full track (no excerpt)\")\n",
    "    else:\n",
    "        # Determine start position\n",
    "        if auto_select_excerpt:\n",
    "            excerpt_start = find_energetic_region(CANONICAL_AUDIO, excerpt_duration_seconds)\n",
    "            print(f\"\ud83c\udfaf Auto-selected energetic region starting at {excerpt_start:.1f}s\")\n",
    "        else:\n",
    "            excerpt_start = excerpt_start_seconds\n",
    "\n",
    "        # Clamp to valid range\n",
    "        max_start = max(0, total_duration - excerpt_duration_seconds)\n",
    "        excerpt_start = min(excerpt_start, max_start)\n",
    "        actual_duration = min(excerpt_duration_seconds, total_duration - excerpt_start)\n",
    "\n",
    "        EXCERPT_AUDIO = extract_excerpt(CANONICAL_AUDIO, excerpt_start, actual_duration)\n",
    "        EXCERPT_START = excerpt_start\n",
    "        EXCERPT_DURATION = actual_duration\n",
    "\n",
    "        print(\"\\n\u2702\ufe0f Excerpt extracted:\")\n",
    "        print(f\"   Start: {EXCERPT_START:.2f}s\")\n",
    "        print(f\"   Duration: {EXCERPT_DURATION:.2f}s\")\n",
    "        print(f\"   End: {EXCERPT_START + EXCERPT_DURATION:.2f}s\")\n",
    "\n",
    "    print(\"\\n\u2705 Audio ready for processing\")\n",
    "    print(f\"   Shape: {EXCERPT_AUDIO.samples.shape}\")\n",
    "    print(f\"   Sample rate: {EXCERPT_AUDIO.sample_rate} Hz\")\n",
    "\n",
    "    # Initialize run ID if not resumed\n",
    "    if RUN_ID is None:\n",
    "        import uuid\n",
    "\n",
    "        RUN_ID = f\"{AUDIO_HASH}_{uuid.uuid4().hex[:8]}\"\n",
    "        print(f\"\\n\ud83c\udd94 Generated run ID: {RUN_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83c\udf9a\ufe0f Stem Separation\n",
    "# @markdown Configure and run Demucs stem separation.\n",
    "\n",
    "# --- Separation Model ---\n",
    "separation_model = (\n",
    "    \"htdemucs_ft\"  # @param [\"htdemucs\", \"htdemucs_ft\", \"htdemucs_6s\", \"mdx_extra\", \"mdx_extra_q\"]\n",
    ")\n",
    "two_stems = \"none\"  # @param [\"none\", \"vocals\", \"drums\", \"bass\", \"other\"]\n",
    "\n",
    "# --- Processing Options ---\n",
    "segment_length = 7.8  # @param {type:\"slider\", min:5.0, max:30.0, step:0.1}\n",
    "overlap = 0.25  # @param {type:\"slider\", min:0.1, max:0.5, step:0.05}\n",
    "shifts = 1  # @param {type:\"slider\", min:0, max:5, step:1}\n",
    "\n",
    "# --- Output Format ---\n",
    "output_format = \"wav\"  # @param [\"wav\", \"mp3\", \"flac\"]\n",
    "mp3_bitrate = 320  # @param [128, 192, 256, 320] {type:\"raw\"}\n",
    "float32_output = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# === Execution ===\n",
    "from pathlib import Path\n",
    "\n",
    "from soundlab.separation import StemSeparator\n",
    "from soundlab.separation.models import DemucsModel, SeparationConfig\n",
    "\n",
    "# Validate input exists\n",
    "if \"AUDIO_PATH\" not in dir() or AUDIO_PATH is None:\n",
    "    raise RuntimeError(\"\u274c No audio file loaded. Please run the Upload cell first.\")\n",
    "\n",
    "print(f\"\ud83c\udfb5 Input: {AUDIO_PATH}\")\n",
    "print(f\"\ud83c\udf9a\ufe0f Model: {separation_model}\")\n",
    "\n",
    "# Build configuration\n",
    "model_enum = DemucsModel(separation_model)\n",
    "two_stems_val = None if two_stems == \"none\" else two_stems\n",
    "\n",
    "sep_config = SeparationConfig(\n",
    "    model=model_enum,\n",
    "    segment_length=segment_length,\n",
    "    overlap=overlap,\n",
    "    shifts=shifts,\n",
    "    two_stems=two_stems_val,\n",
    "    float32=float32_output,\n",
    "    mp3_bitrate=mp3_bitrate if output_format == \"mp3\" else None,\n",
    ")\n",
    "\n",
    "print(\"\\n\u2699\ufe0f Configuration:\")\n",
    "print(f\"   Segment length: {segment_length}s\")\n",
    "print(f\"   Overlap: {overlap}\")\n",
    "print(f\"   Shifts: {shifts}\")\n",
    "print(f\"   Two stems: {two_stems_val or 'disabled'}\")\n",
    "\n",
    "# Initialize separator\n",
    "separator = StemSeparator(config=sep_config)\n",
    "\n",
    "# Run separation\n",
    "print(\"\\n\ud83d\udd04 Separating stems...\")\n",
    "stem_result = separator.separate(\n",
    "    audio_path=AUDIO_PATH,\n",
    "    output_dir=STEMS_DIR,\n",
    ")\n",
    "\n",
    "# Store results for downstream cells\n",
    "STEM_RESULT = stem_result\n",
    "STEM_PATHS = dict(stem_result.stems.items())\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\u2705 Separation complete!\")\n",
    "print(f\"\ud83d\udcc1 Output directory: {STEMS_DIR}\")\n",
    "print(\"\\n\ud83c\udfbc Extracted stems:\")\n",
    "for name, path in STEM_PATHS.items():\n",
    "    size_mb = Path(path).stat().st_size / (1024 * 1024)\n",
    "    print(f\"   {name}: {path.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "if stem_result.vocals:\n",
    "    print(f\"\\n\ud83c\udfa4 Vocals stem: {stem_result.vocals}\")\n",
    "if stem_result.instrumental:\n",
    "    print(f\"\ud83c\udfb8 Instrumental stem: {stem_result.instrumental}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83c\udfaf Candidate Selection (Multi-Strategy Separation)\n",
    "# @markdown Run multiple separation strategies on an excerpt and select the best one.\n",
    "\n",
    "# --- Excerpt Settings ---\n",
    "enable_candidate_selection = True  # @param {type:\"boolean\"}\n",
    "excerpt_start = 30.0  # @param {type:\"number\"}\n",
    "excerpt_duration = 30.0  # @param {type:\"slider\", min:10.0, max:60.0, step:5.0}\n",
    "max_candidates = 3  # @param {type:\"slider\", min:1, max:5, step:1}\n",
    "\n",
    "# --- QA Thresholds ---\n",
    "min_overall_score = 0.7  # @param {type:\"slider\", min:0.5, max:0.95, step:0.05}\n",
    "max_reconstruction_error = 0.15  # @param {type:\"slider\", min:0.05, max:0.30, step:0.05}\n",
    "max_clipping_ratio = 0.01  # @param {type:\"slider\", min:0.001, max:0.05, step:0.005}\n",
    "\n",
    "# --- Full Run Options ---\n",
    "rerun_full_on_best = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# === Execution ===\n",
    "from pathlib import Path\n",
    "\n",
    "from soundlab.pipeline import (\n",
    "    CandidateScore,\n",
    "    PipelineConfig,\n",
    "    QAConfig,\n",
    "    build_candidate_plans,\n",
    "    choose_best_candidate,\n",
    "    init_run,\n",
    "    score_separation,\n",
    ")\n",
    "from soundlab.separation import StemSeparator\n",
    "from soundlab.separation.models import SeparationConfig\n",
    "\n",
    "if not enable_candidate_selection:\n",
    "    print(\"\u2139\ufe0f Candidate selection disabled. Using single-run separation from previous cell.\")\n",
    "    if \"STEM_RESULT\" in dir():\n",
    "        print(f\"\u2705 Using existing stem result: {len(STEM_RESULT.stems)} stems\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f No stem result found. Run the Separation cell first.\")\n",
    "else:\n",
    "    # Validate input\n",
    "    if \"AUDIO_PATH\" not in dir() or AUDIO_PATH is None:\n",
    "        raise RuntimeError(\"\u274c No audio file loaded. Please run the Upload cell first.\")\n",
    "\n",
    "    print(f\"\ud83c\udfb5 Input: {AUDIO_PATH}\")\n",
    "    print(f\"\u23f1\ufe0f Excerpt: {excerpt_start}s - {excerpt_start + excerpt_duration}s\")\n",
    "    print(f\"\ud83c\udfaf Max candidates: {max_candidates}\")\n",
    "\n",
    "    # Build QA config\n",
    "    qa_config = QAConfig(\n",
    "        min_overall_score=min_overall_score,\n",
    "        max_reconstruction_error=max_reconstruction_error,\n",
    "        max_clipping_ratio=max_clipping_ratio,\n",
    "    )\n",
    "\n",
    "    # Build pipeline config\n",
    "    pipeline_config = PipelineConfig(\n",
    "        excerpt_start=excerpt_start,\n",
    "        excerpt_duration=excerpt_duration,\n",
    "        max_candidates=max_candidates,\n",
    "        qa=qa_config,\n",
    "    )\n",
    "\n",
    "    # Generate candidate plans\n",
    "    plans = build_candidate_plans(pipeline_config)\n",
    "    print(f\"\\n\ud83d\udccb Candidate plans ({len(plans)}):\")\n",
    "    for plan in plans:\n",
    "        print(f\"   \u2022 {plan.name}: {plan.notes or 'default settings'}\")\n",
    "\n",
    "    # Initialize run\n",
    "    artifacts = init_run(pipeline_config, Path(AUDIO_PATH), root=RUNS_DIR)\n",
    "    print(f\"\\n\ud83c\udd94 Run ID: {artifacts.run_id}\")\n",
    "\n",
    "    # Run candidates on excerpt\n",
    "    print(f\"\\n\ud83d\udd04 Running {len(plans)} candidates on excerpt...\")\n",
    "    candidate_scores: list[CandidateScore] = []\n",
    "\n",
    "    for i, plan in enumerate(plans, 1):\n",
    "        print(f\"\\n--- Candidate {i}/{len(plans)}: {plan.name} ---\")\n",
    "\n",
    "        # Create separator for this plan\n",
    "        separator = StemSeparator(config=plan.separation)\n",
    "\n",
    "        try:\n",
    "            # Separate excerpt\n",
    "            excerpt_result = separator.separate(\n",
    "                audio_path=AUDIO_PATH,\n",
    "                output_dir=artifacts.cache_dir / f\"excerpt_{plan.name}\",\n",
    "                start_time=excerpt_start,\n",
    "                duration=excerpt_duration,\n",
    "            )\n",
    "\n",
    "            # Score separation quality\n",
    "            qa_result = score_separation(\n",
    "                stems=excerpt_result.stems,\n",
    "                original_path=Path(AUDIO_PATH),\n",
    "                config=qa_config,\n",
    "            )\n",
    "\n",
    "            score = CandidateScore(\n",
    "                name=plan.name,\n",
    "                score=qa_result.score,\n",
    "                metrics=qa_result.metrics,\n",
    "                passed=qa_result.passed,\n",
    "            )\n",
    "            candidate_scores.append(score)\n",
    "\n",
    "            status = \"\u2705 PASS\" if qa_result.passed else \"\u26a0\ufe0f WARN\"\n",
    "            print(f\"   {status} Score: {qa_result.score:.3f}\")\n",
    "            for metric, value in qa_result.metrics.items():\n",
    "                print(f\"      {metric}: {value:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   \u274c Failed: {e}\")\n",
    "            candidate_scores.append(CandidateScore(name=plan.name, score=0.0, passed=False))\n",
    "\n",
    "    # Select best candidate\n",
    "    best = choose_best_candidate(candidate_scores, qa=qa_config)\n",
    "\n",
    "    if best:\n",
    "        print(f\"\\n\ud83c\udfc6 Best candidate: {best.name} (score: {best.score:.3f})\")\n",
    "\n",
    "        # Store for downstream\n",
    "        BEST_CANDIDATE = best\n",
    "        BEST_PLAN = next(p for p in plans if p.name == best.name)\n",
    "\n",
    "        if rerun_full_on_best:\n",
    "            print(\"\\n\ud83d\udd04 Re-running full separation with best candidate...\")\n",
    "\n",
    "            separator = StemSeparator(config=BEST_PLAN.separation)\n",
    "            stem_result = separator.separate(\n",
    "                audio_path=AUDIO_PATH,\n",
    "                output_dir=STEMS_DIR,\n",
    "            )\n",
    "\n",
    "            # Update globals\n",
    "            STEM_RESULT = stem_result\n",
    "            STEM_PATHS = dict(stem_result.stems.items())\n",
    "\n",
    "            print(\"\u2705 Full separation complete!\")\n",
    "            print(f\"\ud83d\udcc1 Output: {STEMS_DIR}\")\n",
    "            for name, path in STEM_PATHS.items():\n",
    "                print(f\"   {name}: {path.name}\")\n",
    "        else:\n",
    "            print(\"\u2139\ufe0f Full re-run disabled. Using excerpt result.\")\n",
    "    else:\n",
    "        print(\"\u274c No candidates passed QA thresholds. Consider adjusting settings.\")\n",
    "\n",
    "    # Summary table\n",
    "    print(\"\\n\ud83d\udcca Candidate Summary:\")\n",
    "    print(f\"{'Name':<15} {'Score':<10} {'Status':<10}\")\n",
    "    print(\"-\" * 35)\n",
    "    for score in sorted(candidate_scores, key=lambda x: x.score, reverse=True):\n",
    "        status = \"PASS\" if score.passed else \"FAIL\"\n",
    "        marker = \"\u2192\" if best and score.name == best.name else \" \"\n",
    "        print(f\"{marker}{score.name:<14} {score.score:<10.3f} {status:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83e\uddf9 Stem Post-Processing\n",
    "# @markdown Clean and prepare stems for transcription. Applies alignment-safe filtering,\n",
    "# @markdown mono conversion for AMT, and clipping detection.\n",
    "\n",
    "# --- Silence Filtering ---\n",
    "silence_threshold = 1e-4  # @param {type:\"number\"}\n",
    "enable_silence_filtering = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# --- Clipping Detection ---\n",
    "clipping_threshold = 0.99  # @param {type:\"number\"}\n",
    "max_clipping_ratio = 0.01  # @param {type:\"number\"}\n",
    "\n",
    "# --- AMT Preparation ---\n",
    "prepare_mono_for_amt = True  # @param {type:\"boolean\"}\n",
    "save_mono_stems = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# === Execution ===\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from soundlab.io import load_audio, save_audio\n",
    "from soundlab.pipeline.postprocess import clean_stems, mono_amt_exports\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from soundlab.core.audio import AudioSegment\n",
    "\n",
    "\n",
    "def detect_clipping(\n",
    "    samples: np.ndarray,\n",
    "    threshold: float = 0.99,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Detect clipping in audio samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples\n",
    "        Audio samples (mono or stereo).\n",
    "    threshold\n",
    "        Amplitude threshold for clipping detection.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Clipping metrics: ratio, peak, and count.\n",
    "    \"\"\"\n",
    "    abs_samples = np.abs(samples)\n",
    "    peak = float(np.max(abs_samples))\n",
    "    clipped_count = int(np.sum(abs_samples >= threshold))\n",
    "    total_samples = samples.size\n",
    "    clipping_ratio = clipped_count / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"peak\": peak,\n",
    "        \"clipped_count\": clipped_count,\n",
    "        \"clipping_ratio\": clipping_ratio,\n",
    "    }\n",
    "\n",
    "\n",
    "def load_stem_arrays(stem_paths: dict[str, Path]) -> dict[str, np.ndarray]:\n",
    "    \"\"\"Load stem audio files into numpy arrays.\"\"\"\n",
    "    stems: dict[str, np.ndarray] = {}\n",
    "    for name, path in stem_paths.items():\n",
    "        audio = load_audio(path)\n",
    "        stems[name] = audio.samples\n",
    "    return stems\n",
    "\n",
    "\n",
    "# Validate prerequisites\n",
    "if \"STEM_PATHS\" not in dir() or not STEM_PATHS:\n",
    "    raise RuntimeError(\"\u274c No stem paths found. Please run the Separation cell first.\")\n",
    "\n",
    "print(f\"\ud83e\uddf9 Post-processing {len(STEM_PATHS)} stems...\")\n",
    "print(f\"   Silence threshold: {silence_threshold}\")\n",
    "print(f\"   Clipping threshold: {clipping_threshold}\")\n",
    "\n",
    "# Load stem arrays\n",
    "print(\"\\n\ud83d\udcc2 Loading stems...\")\n",
    "STEM_ARRAYS = load_stem_arrays(STEM_PATHS)\n",
    "\n",
    "# Check for clipping\n",
    "print(\"\\n\ud83d\udcca Clipping analysis:\")\n",
    "CLIPPING_RESULTS: dict[str, dict[str, float]] = {}\n",
    "any_clipping_issues = False\n",
    "\n",
    "for name, samples in STEM_ARRAYS.items():\n",
    "    clip_info = detect_clipping(samples, threshold=clipping_threshold)\n",
    "    CLIPPING_RESULTS[name] = clip_info\n",
    "\n",
    "    status = \"\u2705\" if clip_info[\"clipping_ratio\"] <= max_clipping_ratio else \"\u26a0\ufe0f\"\n",
    "    if clip_info[\"clipping_ratio\"] > max_clipping_ratio:\n",
    "        any_clipping_issues = True\n",
    "\n",
    "    print(\n",
    "        f\"   {status} {name}: peak={clip_info['peak']:.4f}, \"\n",
    "        f\"clipping={clip_info['clipping_ratio'] * 100:.3f}%\"\n",
    "    )\n",
    "\n",
    "if any_clipping_issues:\n",
    "    print(\n",
    "        \"\\n\u26a0\ufe0f Some stems exceed clipping threshold. Consider re-running separation \"\n",
    "        \"with different settings or applying limiting during export.\"\n",
    "    )\n",
    "\n",
    "# Apply silence filtering\n",
    "if enable_silence_filtering:\n",
    "    print(f\"\\n\ud83d\udd07 Applying silence filtering (threshold={silence_threshold})...\")\n",
    "    STEM_ARRAYS_CLEANED = clean_stems(STEM_ARRAYS, silence_threshold=silence_threshold)\n",
    "    print(\"   \u2705 Silence filtering complete\")\n",
    "else:\n",
    "    STEM_ARRAYS_CLEANED = STEM_ARRAYS\n",
    "    print(\"\\n\u2139\ufe0f Silence filtering disabled\")\n",
    "\n",
    "# Prepare mono stems for AMT\n",
    "if prepare_mono_for_amt:\n",
    "    print(\"\\n\ud83c\udfb9 Preparing mono stems for AMT...\")\n",
    "    MONO_STEMS = mono_amt_exports(STEM_ARRAYS_CLEANED)\n",
    "\n",
    "    # Display mono stem info\n",
    "    for name, samples in MONO_STEMS.items():\n",
    "        print(f\"   {name}: {samples.shape} ({samples.dtype})\")\n",
    "\n",
    "    # Save mono stems if requested\n",
    "    if save_mono_stems:\n",
    "        MONO_STEMS_DIR = STEMS_DIR / \"mono_amt\"\n",
    "        MONO_STEMS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "        MONO_STEM_PATHS: dict[str, Path] = {}\n",
    "        for name, samples in MONO_STEMS.items():\n",
    "            # Infer sample rate from original stem\n",
    "            sr = load_audio(STEM_PATHS[name]).sample_rate\n",
    "            mono_path = MONO_STEMS_DIR / f\"{name}_mono.wav\"\n",
    "            save_audio(samples, mono_path, sample_rate=sr)\n",
    "            MONO_STEM_PATHS[name] = mono_path\n",
    "\n",
    "        print(f\"\\n\ud83d\udcbe Saved mono stems to: {MONO_STEMS_DIR}\")\n",
    "        for name, path in MONO_STEM_PATHS.items():\n",
    "            size_kb = path.stat().st_size / 1024\n",
    "            print(f\"   {name}: {path.name} ({size_kb:.1f} KB)\")\n",
    "else:\n",
    "    MONO_STEMS = {}\n",
    "    MONO_STEM_PATHS = {}\n",
    "    print(\"\\n\u2139\ufe0f Mono AMT preparation disabled\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\u2705 Post-processing complete!\")\n",
    "print(f\"   Cleaned stems: {len(STEM_ARRAYS_CLEANED)}\")\n",
    "print(f\"   Mono AMT stems: {len(MONO_STEMS)}\")\n",
    "print(f\"   Clipping issues: {'Yes' if any_clipping_issues else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83c\udfb9 Transcription Routing\n",
    "# @markdown Configure per-stem transcription backends and run audio-to-MIDI conversion.\n",
    "# @markdown Each stem type can use different thresholds optimized for that instrument.\n",
    "\n",
    "# --- Stem Selection ---\n",
    "transcribe_vocals = False  # @param {type:\"boolean\"}\n",
    "transcribe_drums = False  # @param {type:\"boolean\"}\n",
    "transcribe_bass = True  # @param {type:\"boolean\"}\n",
    "transcribe_other = True  # @param {type:\"boolean\"}\n",
    "transcribe_piano = True  # @param {type:\"boolean\"}\n",
    "transcribe_guitar = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# --- Global Thresholds ---\n",
    "onset_threshold = 0.5  # @param {type:\"slider\", min:0.1, max:0.9, step:0.05}\n",
    "frame_threshold = 0.3  # @param {type:\"slider\", min:0.1, max:0.9, step:0.05}\n",
    "min_note_length = 0.058  # @param {type:\"number\"}\n",
    "\n",
    "# --- Frequency Ranges ---\n",
    "bass_min_freq = 32.7  # @param {type:\"number\"}\n",
    "bass_max_freq = 500.0  # @param {type:\"number\"}\n",
    "default_min_freq = 32.7  # @param {type:\"number\"}\n",
    "default_max_freq = 2093.0  # @param {type:\"number\"}\n",
    "\n",
    "# --- Fallback Behavior ---\n",
    "enable_fallback = True  # @param {type:\"boolean\"}\n",
    "fallback_lower_threshold = 0.1  # @param {type:\"number\"}\n",
    "\n",
    "# === Execution ===\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "from soundlab.transcription import MIDIResult, MIDITranscriber, TranscriptionConfig\n",
    "\n",
    "# Stem routing configuration\n",
    "STEM_ROUTING: dict[str, dict[str, bool | float]] = {\n",
    "    \"vocals\": {\n",
    "        \"enabled\": transcribe_vocals,\n",
    "        \"min_freq\": default_min_freq,\n",
    "        \"max_freq\": default_max_freq,\n",
    "    },\n",
    "    \"drums\": {\n",
    "        \"enabled\": transcribe_drums,\n",
    "        \"min_freq\": default_min_freq,\n",
    "        \"max_freq\": default_max_freq,\n",
    "    },\n",
    "    \"bass\": {\"enabled\": transcribe_bass, \"min_freq\": bass_min_freq, \"max_freq\": bass_max_freq},\n",
    "    \"other\": {\n",
    "        \"enabled\": transcribe_other,\n",
    "        \"min_freq\": default_min_freq,\n",
    "        \"max_freq\": default_max_freq,\n",
    "    },\n",
    "    \"piano\": {\n",
    "        \"enabled\": transcribe_piano,\n",
    "        \"min_freq\": default_min_freq,\n",
    "        \"max_freq\": default_max_freq,\n",
    "    },\n",
    "    \"guitar\": {\n",
    "        \"enabled\": transcribe_guitar,\n",
    "        \"min_freq\": default_min_freq,\n",
    "        \"max_freq\": default_max_freq,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TranscriptionResult:\n",
    "    \"\"\"Result from transcription routing for a single stem.\"\"\"\n",
    "\n",
    "    stem_name: str\n",
    "    midi_result: MIDIResult | None = None\n",
    "    confidence: float = 0.0\n",
    "    fallback_used: bool = False\n",
    "    error: str | None = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TranscriptionBatch:\n",
    "    \"\"\"Batch transcription results with confidence matrix.\"\"\"\n",
    "\n",
    "    results: dict[str, TranscriptionResult] = field(default_factory=dict)\n",
    "    total_notes: int = 0\n",
    "    successful_stems: int = 0\n",
    "    failed_stems: int = 0\n",
    "\n",
    "    @property\n",
    "    def confidence_matrix(self) -> dict[str, float]:\n",
    "        \"\"\"Return confidence scores for all stems.\"\"\"\n",
    "        return {name: r.confidence for name, r in self.results.items()}\n",
    "\n",
    "\n",
    "def build_stem_config(\n",
    "    stem_name: str,\n",
    "    routing: dict[str, dict[str, bool | float]],\n",
    "    onset_thresh: float,\n",
    "    frame_thresh: float,\n",
    "    min_note_len: float,\n",
    ") -> TranscriptionConfig | None:\n",
    "    \"\"\"Build transcription config for a stem based on routing rules.\"\"\"\n",
    "    stem_config = routing.get(stem_name)\n",
    "    if not stem_config or not stem_config.get(\"enabled\", False):\n",
    "        return None\n",
    "\n",
    "    return TranscriptionConfig(\n",
    "        onset_thresh=onset_thresh,\n",
    "        frame_thresh=frame_thresh,\n",
    "        min_note_length=min_note_len,\n",
    "        min_freq=float(stem_config.get(\"min_freq\", default_min_freq)),\n",
    "        max_freq=float(stem_config.get(\"max_freq\", default_max_freq)),\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_confidence(result: MIDIResult) -> float:\n",
    "    \"\"\"\n",
    "    Compute transcription confidence based on note density and consistency.\n",
    "\n",
    "    Higher confidence for:\n",
    "    - More notes (up to a point)\n",
    "    - Consistent note velocities\n",
    "    - Reasonable note durations\n",
    "    \"\"\"\n",
    "    notes = result.notes\n",
    "    if not notes:\n",
    "        return 0.0\n",
    "\n",
    "    note_count = len(notes)\n",
    "\n",
    "    # Note count factor (saturates around 50-100 notes)\n",
    "    count_factor = min(1.0, note_count / 50.0)\n",
    "\n",
    "    # Velocity variance factor (lower variance = higher confidence)\n",
    "    velocities = [n.velocity for n in notes]\n",
    "    vel_mean = sum(velocities) / len(velocities)\n",
    "    vel_var = sum((v - vel_mean) ** 2 for v in velocities) / len(velocities)\n",
    "    vel_factor = max(0.0, 1.0 - (vel_var / 2000.0))  # Normalize variance\n",
    "\n",
    "    # Duration consistency factor\n",
    "    durations = [n.end - n.start for n in notes]\n",
    "    dur_mean = sum(durations) / len(durations)\n",
    "    dur_var = sum((d - dur_mean) ** 2 for d in durations) / len(durations)\n",
    "    dur_factor = max(0.0, 1.0 - (dur_var / 1.0))  # Normalize\n",
    "\n",
    "    # Combined confidence\n",
    "    confidence = count_factor * 0.4 + vel_factor * 0.3 + dur_factor * 0.3\n",
    "    return min(1.0, max(0.0, confidence))\n",
    "\n",
    "\n",
    "def transcribe_stem_with_fallback(\n",
    "    stem_path: Path,\n",
    "    stem_name: str,\n",
    "    config: TranscriptionConfig,\n",
    "    output_dir: Path,\n",
    "    enable_fallback: bool = True,\n",
    "    fallback_delta: float = 0.1,\n",
    ") -> TranscriptionResult:\n",
    "    \"\"\"\n",
    "    Transcribe a single stem with optional fallback on low confidence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stem_path\n",
    "        Path to the stem audio file.\n",
    "    stem_name\n",
    "        Name of the stem.\n",
    "    config\n",
    "        Transcription configuration.\n",
    "    output_dir\n",
    "        Directory to save MIDI output.\n",
    "    enable_fallback\n",
    "        Whether to retry with lower thresholds on failure.\n",
    "    fallback_delta\n",
    "        Amount to lower thresholds on fallback.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TranscriptionResult\n",
    "        Transcription result with confidence and fallback status.\n",
    "    \"\"\"\n",
    "    transcriber = MIDITranscriber(config=config)\n",
    "    stem_output = output_dir / stem_name\n",
    "    stem_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        midi_result = transcriber.transcribe(stem_path, stem_output)\n",
    "        confidence = compute_confidence(midi_result)\n",
    "\n",
    "        # Check if fallback needed (low note count or confidence)\n",
    "        if enable_fallback and len(midi_result.notes) < 5 and confidence < 0.3:\n",
    "            # Retry with lower thresholds\n",
    "            fallback_config = TranscriptionConfig(\n",
    "                onset_thresh=max(0.1, config.onset_thresh - fallback_delta),\n",
    "                frame_thresh=max(0.1, config.frame_thresh - fallback_delta),\n",
    "                min_note_length=config.min_note_length,\n",
    "                min_freq=config.min_freq,\n",
    "                max_freq=config.max_freq,\n",
    "            )\n",
    "\n",
    "            fallback_transcriber = MIDITranscriber(config=fallback_config)\n",
    "            fallback_result = fallback_transcriber.transcribe(stem_path, stem_output)\n",
    "            fallback_confidence = compute_confidence(fallback_result)\n",
    "\n",
    "            if len(fallback_result.notes) > len(midi_result.notes):\n",
    "                return TranscriptionResult(\n",
    "                    stem_name=stem_name,\n",
    "                    midi_result=fallback_result,\n",
    "                    confidence=fallback_confidence,\n",
    "                    fallback_used=True,\n",
    "                )\n",
    "\n",
    "        return TranscriptionResult(\n",
    "            stem_name=stem_name,\n",
    "            midi_result=midi_result,\n",
    "            confidence=confidence,\n",
    "            fallback_used=False,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return TranscriptionResult(\n",
    "            stem_name=stem_name,\n",
    "            midi_result=None,\n",
    "            confidence=0.0,\n",
    "            error=str(e),\n",
    "        )\n",
    "\n",
    "\n",
    "# Validate prerequisites\n",
    "if \"MONO_STEM_PATHS\" not in dir() or not MONO_STEM_PATHS:\n",
    "    # Fall back to original stems if mono not available\n",
    "    if \"STEM_PATHS\" not in dir() or not STEM_PATHS:\n",
    "        raise RuntimeError(\"\u274c No stems found. Please run the Separation cell first.\")\n",
    "    TRANSCRIPTION_INPUTS = STEM_PATHS\n",
    "    print(\"\u2139\ufe0f Using original stereo stems (mono not available)\")\n",
    "else:\n",
    "    TRANSCRIPTION_INPUTS = MONO_STEM_PATHS\n",
    "    print(\"\u2705 Using mono stems for transcription\")\n",
    "\n",
    "# Build enabled stems list\n",
    "enabled_stems = [name for name, cfg in STEM_ROUTING.items() if cfg.get(\"enabled\", False)]\n",
    "available_stems = [name for name in enabled_stems if name in TRANSCRIPTION_INPUTS]\n",
    "\n",
    "print(f\"\\n\ud83c\udfb9 Transcription routing:\")\n",
    "print(f\"   Enabled stems: {enabled_stems}\")\n",
    "print(f\"   Available stems: {available_stems}\")\n",
    "print(f\"   Global onset threshold: {onset_threshold}\")\n",
    "print(f\"   Global frame threshold: {frame_threshold}\")\n",
    "\n",
    "if not available_stems:\n",
    "    print(\"\\n\u26a0\ufe0f No stems selected for transcription. Enable at least one stem above.\")\n",
    "else:\n",
    "    # Run transcription\n",
    "    print(f\"\\n\ud83d\udd04 Transcribing {len(available_stems)} stems...\")\n",
    "\n",
    "    TRANSCRIPTION_BATCH = TranscriptionBatch()\n",
    "\n",
    "    for stem_name in available_stems:\n",
    "        stem_path = TRANSCRIPTION_INPUTS[stem_name]\n",
    "        config = build_stem_config(\n",
    "            stem_name,\n",
    "            STEM_ROUTING,\n",
    "            onset_threshold,\n",
    "            frame_threshold,\n",
    "            min_note_length,\n",
    "        )\n",
    "\n",
    "        if config is None:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- {stem_name} ---\")\n",
    "        print(f\"   Path: {stem_path}\")\n",
    "        print(f\"   Freq range: {config.min_freq:.1f} - {config.max_freq:.1f} Hz\")\n",
    "\n",
    "        result = transcribe_stem_with_fallback(\n",
    "            stem_path=stem_path,\n",
    "            stem_name=stem_name,\n",
    "            config=config,\n",
    "            output_dir=MIDI_DIR,\n",
    "            enable_fallback=enable_fallback,\n",
    "            fallback_delta=fallback_lower_threshold,\n",
    "        )\n",
    "\n",
    "        TRANSCRIPTION_BATCH.results[stem_name] = result\n",
    "\n",
    "        if result.error:\n",
    "            TRANSCRIPTION_BATCH.failed_stems += 1\n",
    "            print(f\"   \u274c Error: {result.error}\")\n",
    "        elif result.midi_result:\n",
    "            TRANSCRIPTION_BATCH.successful_stems += 1\n",
    "            TRANSCRIPTION_BATCH.total_notes += len(result.midi_result.notes)\n",
    "            status = \"\u2705\"\n",
    "            if result.fallback_used:\n",
    "                status = \"\u26a0\ufe0f (fallback)\"\n",
    "            print(\n",
    "                f\"   {status} Notes: {len(result.midi_result.notes)}, \"\n",
    "                f\"Confidence: {result.confidence:.2f}\"\n",
    "            )\n",
    "            print(f\"   MIDI: {result.midi_result.path}\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"\ud83d\udcca Transcription Summary:\")\n",
    "    print(f\"   Successful: {TRANSCRIPTION_BATCH.successful_stems}\")\n",
    "    print(f\"   Failed: {TRANSCRIPTION_BATCH.failed_stems}\")\n",
    "    print(f\"   Total notes: {TRANSCRIPTION_BATCH.total_notes}\")\n",
    "\n",
    "    # Confidence matrix\n",
    "    print(\"\\n\ud83c\udfaf Confidence Matrix:\")\n",
    "    for stem, conf in TRANSCRIPTION_BATCH.confidence_matrix.items():\n",
    "        bar = \"\u2588\" * int(conf * 20) + \"\u2591\" * (20 - int(conf * 20))\n",
    "        print(f\"   {stem:<10} [{bar}] {conf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83c\udfbc MIDI Cleanup\n",
    "# @markdown Clean up transcribed MIDI: filter short notes, detect tempo, apply quantization,\n",
    "# @markdown and assign General MIDI program numbers to stems.\n",
    "\n",
    "# --- Note Filtering ---\n",
    "min_note_duration = 0.02  # @param {type:\"number\"}\n",
    "min_velocity = 1  # @param {type:\"slider\", min:1, max:127, step:1}\n",
    "filter_short_notes = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# --- Tempo Detection ---\n",
    "detect_tempo_from_audio = True  # @param {type:\"boolean\"}\n",
    "fallback_bpm = 120.0  # @param {type:\"number\"}\n",
    "\n",
    "# --- Quantization ---\n",
    "enable_quantization = True  # @param {type:\"boolean\"}\n",
    "quantize_strength = 0.5  # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
    "quantize_grid = \"1/16\"  # @param [\"1/4\", \"1/8\", \"1/16\", \"1/32\"]\n",
    "\n",
    "# --- Program Mapping ---\n",
    "enable_program_mapping = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# === Execution ===\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from soundlab.analysis.tempo import detect_tempo\n",
    "from soundlab.io import load_audio\n",
    "from soundlab.io.midi_io import MIDIData, MIDINote, TimeSignature, save_midi\n",
    "from soundlab.pipeline.postprocess import cleanup_midi_notes\n",
    "from soundlab.transcription.models import NoteEvent\n",
    "\n",
    "# General MIDI program mapping for stems\n",
    "GM_PROGRAMS: dict[str, int] = {\n",
    "    \"vocals\": 54,  # Voice Oohs\n",
    "    \"drums\": 0,  # Drums on channel 10 (program ignored)\n",
    "    \"bass\": 33,  # Finger Bass\n",
    "    \"other\": 0,  # Acoustic Grand Piano\n",
    "    \"piano\": 0,  # Acoustic Grand Piano\n",
    "    \"guitar\": 25,  # Acoustic Guitar (steel)\n",
    "}\n",
    "\n",
    "# Grid values in beats\n",
    "GRID_VALUES: dict[str, float] = {\n",
    "    \"1/4\": 1.0,\n",
    "    \"1/8\": 0.5,\n",
    "    \"1/16\": 0.25,\n",
    "    \"1/32\": 0.125,\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CleanedMIDI:\n",
    "    \"\"\"Container for cleaned MIDI data.\"\"\"\n",
    "\n",
    "    stem_name: str\n",
    "    original_notes: int\n",
    "    cleaned_notes: int\n",
    "    midi_data: MIDIData\n",
    "    output_path: Path | None = None\n",
    "    tempo_bpm: float = 120.0\n",
    "    quantized: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MIDICleanupBatch:\n",
    "    \"\"\"Batch of cleaned MIDI results.\"\"\"\n",
    "\n",
    "    results: dict[str, CleanedMIDI] = field(default_factory=dict)\n",
    "    detected_tempo: float = 120.0\n",
    "    tempo_confidence: float = 0.0\n",
    "    total_original_notes: int = 0\n",
    "    total_cleaned_notes: int = 0\n",
    "\n",
    "    @property\n",
    "    def notes_removed(self) -> int:\n",
    "        return self.total_original_notes - self.total_cleaned_notes\n",
    "\n",
    "\n",
    "def quantize_notes(\n",
    "    notes: list[NoteEvent],\n",
    "    bpm: float,\n",
    "    grid: float,\n",
    "    strength: float,\n",
    ") -> list[NoteEvent]:\n",
    "    \"\"\"\n",
    "    Apply soft quantization to notes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    notes\n",
    "        List of note events to quantize.\n",
    "    bpm\n",
    "        Tempo in beats per minute.\n",
    "    grid\n",
    "        Grid size in beats.\n",
    "    strength\n",
    "        Quantization strength (0.0 = no change, 1.0 = full snap).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[NoteEvent]\n",
    "        Quantized notes.\n",
    "    \"\"\"\n",
    "    if strength <= 0 or not notes:\n",
    "        return notes\n",
    "\n",
    "    # Convert grid to seconds\n",
    "    beat_duration = 60.0 / bpm\n",
    "    grid_seconds = grid * beat_duration\n",
    "\n",
    "    quantized: list[NoteEvent] = []\n",
    "    for note in notes:\n",
    "        # Find nearest grid point for start\n",
    "        grid_index = round(note.start / grid_seconds)\n",
    "        snapped_start = grid_index * grid_seconds\n",
    "\n",
    "        # Apply strength (blend between original and snapped)\n",
    "        new_start = note.start + (snapped_start - note.start) * strength\n",
    "\n",
    "        # Preserve duration\n",
    "        duration = note.end - note.start\n",
    "        new_end = new_start + duration\n",
    "\n",
    "        quantized.append(\n",
    "            NoteEvent(\n",
    "                start=max(0.0, new_start),\n",
    "                end=max(new_start + 0.001, new_end),\n",
    "                pitch=note.pitch,\n",
    "                velocity=note.velocity,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return quantized\n",
    "\n",
    "\n",
    "def notes_to_midi_data(\n",
    "    notes: list[NoteEvent],\n",
    "    tempo: float,\n",
    "    time_signature: TimeSignature | None = None,\n",
    ") -> MIDIData:\n",
    "    \"\"\"\n",
    "    Convert NoteEvents to MIDIData.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    notes\n",
    "        List of note events.\n",
    "    tempo\n",
    "        Tempo in BPM.\n",
    "    time_signature\n",
    "        Optional time signature.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MIDIData\n",
    "        MIDI data structure.\n",
    "    \"\"\"\n",
    "    midi_notes = [\n",
    "        MIDINote(\n",
    "            pitch=note.pitch,\n",
    "            start_seconds=note.start,\n",
    "            end_seconds=note.end,\n",
    "            velocity=note.velocity,\n",
    "        )\n",
    "        for note in notes\n",
    "    ]\n",
    "\n",
    "    return MIDIData(\n",
    "        notes=midi_notes,\n",
    "        tempo=tempo,\n",
    "        time_signature=time_signature or TimeSignature(numerator=4, denominator=4),\n",
    "    )\n",
    "\n",
    "\n",
    "def cleanup_stem_midi(\n",
    "    stem_name: str,\n",
    "    midi_result: \"MIDIResult\",\n",
    "    tempo: float,\n",
    "    min_duration: float,\n",
    "    min_vel: int,\n",
    "    enable_filter: bool,\n",
    "    enable_quant: bool,\n",
    "    quant_strength: float,\n",
    "    quant_grid: float,\n",
    "    output_dir: Path,\n",
    ") -> CleanedMIDI:\n",
    "    \"\"\"\n",
    "    Apply cleanup pipeline to a stem's MIDI result.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stem_name\n",
    "        Name of the stem.\n",
    "    midi_result\n",
    "        Transcription result.\n",
    "    tempo\n",
    "        Detected or fallback tempo.\n",
    "    min_duration\n",
    "        Minimum note duration in seconds.\n",
    "    min_vel\n",
    "        Minimum velocity threshold.\n",
    "    enable_filter\n",
    "        Whether to filter short/quiet notes.\n",
    "    enable_quant\n",
    "        Whether to apply quantization.\n",
    "    quant_strength\n",
    "        Quantization strength.\n",
    "    quant_grid\n",
    "        Quantization grid in beats.\n",
    "    output_dir\n",
    "        Directory to save cleaned MIDI.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CleanedMIDI\n",
    "        Cleaned MIDI result.\n",
    "    \"\"\"\n",
    "    original_notes = midi_result.notes\n",
    "    original_count = len(original_notes)\n",
    "\n",
    "    # Step 1: Filter short/quiet notes\n",
    "    if enable_filter:\n",
    "        filtered_notes = cleanup_midi_notes(\n",
    "            original_notes,\n",
    "            min_duration=min_duration,\n",
    "            min_velocity=min_vel,\n",
    "        )\n",
    "    else:\n",
    "        filtered_notes = list(original_notes)\n",
    "\n",
    "    # Step 2: Quantize\n",
    "    if enable_quant and quant_strength > 0:\n",
    "        quantized_notes = quantize_notes(filtered_notes, tempo, quant_grid, quant_strength)\n",
    "        was_quantized = True\n",
    "    else:\n",
    "        quantized_notes = filtered_notes\n",
    "        was_quantized = False\n",
    "\n",
    "    # Step 3: Convert to MIDIData\n",
    "    midi_data = notes_to_midi_data(quantized_notes, tempo)\n",
    "\n",
    "    # Step 4: Save cleaned MIDI\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = output_dir / f\"{stem_name}_cleaned.mid\"\n",
    "    save_midi(midi_data, output_path)\n",
    "\n",
    "    return CleanedMIDI(\n",
    "        stem_name=stem_name,\n",
    "        original_notes=original_count,\n",
    "        cleaned_notes=len(quantized_notes),\n",
    "        midi_data=midi_data,\n",
    "        output_path=output_path,\n",
    "        tempo_bpm=tempo,\n",
    "        quantized=was_quantized,\n",
    "    )\n",
    "\n",
    "\n",
    "# Validate prerequisites\n",
    "if \"TRANSCRIPTION_BATCH\" not in dir() or not TRANSCRIPTION_BATCH.results:\n",
    "    raise RuntimeError(\n",
    "        \"\u274c No transcription results found. Please run the Transcription cell first.\"\n",
    "    )\n",
    "\n",
    "print(f\"\ud83c\udfbc MIDI Cleanup Pipeline\")\n",
    "print(f\"   Min note duration: {min_note_duration}s\")\n",
    "print(f\"   Min velocity: {min_velocity}\")\n",
    "print(f\"   Quantization: {'enabled' if enable_quantization else 'disabled'}\")\n",
    "if enable_quantization:\n",
    "    print(f\"   Quantize grid: {quantize_grid}, strength: {quantize_strength}\")\n",
    "\n",
    "# Step 1: Detect tempo from audio\n",
    "CLEANUP_BATCH = MIDICleanupBatch()\n",
    "\n",
    "if detect_tempo_from_audio:\n",
    "    print(\"\\n\u23f1\ufe0f Detecting tempo from audio...\")\n",
    "\n",
    "    # Use canonical audio if available, otherwise try to load from source\n",
    "    if \"CANONICAL_AUDIO\" in dir() and CANONICAL_AUDIO is not None:\n",
    "        tempo_audio = CANONICAL_AUDIO\n",
    "    elif \"AUDIO_PATH\" in dir() and AUDIO_PATH is not None:\n",
    "        tempo_audio = load_audio(AUDIO_PATH)\n",
    "    else:\n",
    "        tempo_audio = None\n",
    "\n",
    "    if tempo_audio is not None:\n",
    "        try:\n",
    "            tempo_result = detect_tempo(tempo_audio.samples, tempo_audio.sample_rate)\n",
    "            CLEANUP_BATCH.detected_tempo = tempo_result.bpm\n",
    "            CLEANUP_BATCH.tempo_confidence = tempo_result.confidence\n",
    "            print(\n",
    "                f\"   Detected BPM: {tempo_result.bpm:.1f} (confidence: {tempo_result.confidence:.2f})\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            CLEANUP_BATCH.detected_tempo = fallback_bpm\n",
    "            print(f\"   \u26a0\ufe0f Tempo detection failed: {e}\")\n",
    "            print(f\"   Using fallback: {fallback_bpm} BPM\")\n",
    "    else:\n",
    "        CLEANUP_BATCH.detected_tempo = fallback_bpm\n",
    "        print(f\"   \u26a0\ufe0f No audio available for tempo detection\")\n",
    "        print(f\"   Using fallback: {fallback_bpm} BPM\")\n",
    "else:\n",
    "    CLEANUP_BATCH.detected_tempo = fallback_bpm\n",
    "    print(f\"\\n\u2139\ufe0f Using manual BPM: {fallback_bpm}\")\n",
    "\n",
    "# Step 2: Process each transcribed stem\n",
    "grid_beats = GRID_VALUES.get(quantize_grid, 0.25)\n",
    "CLEANED_MIDI_DIR = MIDI_DIR / \"cleaned\"\n",
    "\n",
    "print(f\"\\n\ud83d\udd04 Cleaning {len(TRANSCRIPTION_BATCH.results)} MIDI files...\")\n",
    "\n",
    "for stem_name, result in TRANSCRIPTION_BATCH.results.items():\n",
    "    if result.midi_result is None:\n",
    "        print(f\"\\n--- {stem_name} ---\")\n",
    "        print(f\"   \u26a0\ufe0f Skipped (no MIDI result)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- {stem_name} ---\")\n",
    "\n",
    "    cleaned = cleanup_stem_midi(\n",
    "        stem_name=stem_name,\n",
    "        midi_result=result.midi_result,\n",
    "        tempo=CLEANUP_BATCH.detected_tempo,\n",
    "        min_duration=min_note_duration,\n",
    "        min_vel=min_velocity,\n",
    "        enable_filter=filter_short_notes,\n",
    "        enable_quant=enable_quantization,\n",
    "        quant_strength=quantize_strength,\n",
    "        quant_grid=grid_beats,\n",
    "        output_dir=CLEANED_MIDI_DIR,\n",
    "    )\n",
    "\n",
    "    CLEANUP_BATCH.results[stem_name] = cleaned\n",
    "    CLEANUP_BATCH.total_original_notes += cleaned.original_notes\n",
    "    CLEANUP_BATCH.total_cleaned_notes += cleaned.cleaned_notes\n",
    "\n",
    "    removed = cleaned.original_notes - cleaned.cleaned_notes\n",
    "    status = \"\u2705\" if cleaned.cleaned_notes > 0 else \"\u26a0\ufe0f\"\n",
    "    quant_label = \" (quantized)\" if cleaned.quantized else \"\"\n",
    "\n",
    "    print(\n",
    "        f\"   {status} Notes: {cleaned.original_notes} \u2192 {cleaned.cleaned_notes} (-{removed}){quant_label}\"\n",
    "    )\n",
    "    print(f\"   Output: {cleaned.output_path}\")\n",
    "\n",
    "# Step 3: Display program mapping info\n",
    "if enable_program_mapping:\n",
    "    print(\"\\n\ud83c\udfb9 General MIDI Program Mapping:\")\n",
    "    for stem_name in CLEANUP_BATCH.results:\n",
    "        program = GM_PROGRAMS.get(stem_name, 0)\n",
    "        channel = 10 if stem_name == \"drums\" else 1\n",
    "        print(f\"   {stem_name}: Program {program}, Channel {channel}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\u2705 MIDI Cleanup Complete!\")\n",
    "print(f\"   Tempo: {CLEANUP_BATCH.detected_tempo:.1f} BPM\")\n",
    "print(f\"   Total notes: {CLEANUP_BATCH.total_original_notes} \u2192 {CLEANUP_BATCH.total_cleaned_notes}\")\n",
    "print(f\"   Notes removed: {CLEANUP_BATCH.notes_removed}\")\n",
    "print(f\"   Output directory: {CLEANED_MIDI_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83d\udcca QA Dashboard + Preview\n",
    "# @markdown Interactive dashboard for reviewing separation and transcription results.\n",
    "# @markdown Includes audio previews, QA metrics, and rerun controls.\n",
    "\n",
    "# === Execution ===\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from soundlab.core.audio import AudioSegment\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "\n",
    "def load_audio_for_preview(path: Path | str) -> tuple[int, np.ndarray] | None:\n",
    "    \"\"\"Load audio file for Gradio preview.\"\"\"\n",
    "    try:\n",
    "        import soundfile as sf\n",
    "\n",
    "        data, sr = sf.read(str(path), dtype=\"float32\")\n",
    "        # Gradio expects (sr, data) with data as numpy array\n",
    "        return (sr, data)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def compute_residual(\n",
    "    mix_path: Path | str,\n",
    "    stem_paths: dict[str, Path],\n",
    ") -> tuple[int, np.ndarray] | None:\n",
    "    \"\"\"Compute mix - sum(stems) residual for QA visualization.\"\"\"\n",
    "    try:\n",
    "        import soundfile as sf\n",
    "\n",
    "        mix, sr = sf.read(str(mix_path), dtype=\"float32\")\n",
    "\n",
    "        total = np.zeros_like(mix)\n",
    "        for stem_path in stem_paths.values():\n",
    "            stem_audio, _ = sf.read(str(stem_path), dtype=\"float32\")\n",
    "            if stem_audio.shape == mix.shape:\n",
    "                total = total + stem_audio\n",
    "            elif stem_audio.shape[0] == mix.shape[0]:\n",
    "                # Match mono/stereo\n",
    "                if mix.ndim == 2 and stem_audio.ndim == 1:\n",
    "                    stem_audio = np.column_stack([stem_audio, stem_audio])\n",
    "                elif mix.ndim == 1 and stem_audio.ndim == 2:\n",
    "                    stem_audio = np.mean(stem_audio, axis=1)\n",
    "                total = total + stem_audio\n",
    "\n",
    "        residual = mix - total\n",
    "        return (sr, residual)\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Residual computation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def format_qa_metrics(metrics: dict[str, float]) -> str:\n",
    "    \"\"\"Format QA metrics as a markdown table.\"\"\"\n",
    "    if not metrics:\n",
    "        return \"No metrics available\"\n",
    "\n",
    "    rows = [\"| Metric | Value | Status |\", \"|--------|-------|--------|\"]\n",
    "    thresholds = {\n",
    "        \"reconstruction_error\": (0.15, \"lower is better\"),\n",
    "        \"spectral_flatness\": (0.1, \"higher is better\"),\n",
    "        \"clipping_ratio\": (0.01, \"lower is better\"),\n",
    "        \"stereo_coherence\": (0.2, \"higher is better\"),\n",
    "        \"leakage_ratio\": (0.2, \"lower is better\"),\n",
    "    }\n",
    "\n",
    "    for name, value in metrics.items():\n",
    "        if name in thresholds:\n",
    "            thresh, direction = thresholds[name]\n",
    "            if \"lower\" in direction:\n",
    "                status = \"\u2705\" if value <= thresh else \"\u26a0\ufe0f\"\n",
    "            else:\n",
    "                status = \"\u2705\" if value >= thresh else \"\u26a0\ufe0f\"\n",
    "        else:\n",
    "            status = \"\u2139\ufe0f\"\n",
    "        rows.append(f\"| {name.replace('_', ' ').title()} | {value:.4f} | {status} |\")\n",
    "\n",
    "    return \"\\n\".join(rows)\n",
    "\n",
    "\n",
    "def format_candidate_table(candidates: list[dict]) -> str:\n",
    "    \"\"\"Format candidate scores as a comparison table.\"\"\"\n",
    "    if not candidates:\n",
    "        return \"No candidates to display\"\n",
    "\n",
    "    rows = [\"| Candidate | Score | Status |\", \"|-----------|-------|--------|\"]\n",
    "    for c in sorted(candidates, key=lambda x: x.get(\"score\", 0), reverse=True):\n",
    "        status = \"\u2705 PASS\" if c.get(\"passed\", False) else \"\u274c FAIL\"\n",
    "        rows.append(f\"| {c.get('name', 'Unknown')} | {c.get('score', 0):.3f} | {status} |\")\n",
    "\n",
    "    return \"\\n\".join(rows)\n",
    "\n",
    "\n",
    "# --- Dashboard Builder ---\n",
    "\n",
    "\n",
    "def build_qa_dashboard():\n",
    "    \"\"\"Build the QA dashboard interface.\"\"\"\n",
    "    with gr.Blocks(theme=gr.themes.Soft()) as dashboard:\n",
    "        gr.Markdown(\"## \ud83d\udcca QA Dashboard + Preview\")\n",
    "        gr.Markdown(\"Review separation quality, preview stems/MIDI, and re-run if needed.\")\n",
    "\n",
    "        # --- Status Section ---\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                run_id_display = gr.Textbox(\n",
    "                    label=\"Run ID\",\n",
    "                    value=RUN_ID if \"RUN_ID\" in dir() else \"N/A\",\n",
    "                    interactive=False,\n",
    "                )\n",
    "            with gr.Column(scale=1):\n",
    "                status_display = gr.Textbox(\n",
    "                    label=\"Status\",\n",
    "                    value=\"Ready\" if \"STEM_RESULT\" in dir() else \"No stems available\",\n",
    "                    interactive=False,\n",
    "                )\n",
    "\n",
    "        # --- QA Metrics Section ---\n",
    "        gr.Markdown(\"### \ud83d\udcc8 Quality Metrics\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                stem_qa_markdown = gr.Markdown(\n",
    "                    value=\"Run separation first to see QA metrics.\",\n",
    "                    label=\"Stem QA\",\n",
    "                )\n",
    "            with gr.Column(scale=1):\n",
    "                midi_qa_markdown = gr.Markdown(\n",
    "                    value=\"Run transcription to see MIDI QA metrics.\",\n",
    "                    label=\"MIDI QA\",\n",
    "                )\n",
    "\n",
    "        # --- Audio Preview Section ---\n",
    "        gr.Markdown(\"### \ud83d\udd0a Audio Previews\")\n",
    "\n",
    "        with gr.Tabs() as audio_tabs:\n",
    "            with gr.TabItem(\"\ud83c\udfb5 Original Mix\"):\n",
    "                mix_audio = gr.Audio(\n",
    "                    label=\"Original Mix\",\n",
    "                    interactive=False,\n",
    "                )\n",
    "\n",
    "            with gr.TabItem(\"\ud83c\udfa4 Vocals\"):\n",
    "                vocals_audio = gr.Audio(label=\"Vocals Stem\", interactive=False)\n",
    "\n",
    "            with gr.TabItem(\"\ud83e\udd41 Drums\"):\n",
    "                drums_audio = gr.Audio(label=\"Drums Stem\", interactive=False)\n",
    "\n",
    "            with gr.TabItem(\"\ud83c\udfb8 Bass\"):\n",
    "                bass_audio = gr.Audio(label=\"Bass Stem\", interactive=False)\n",
    "\n",
    "            with gr.TabItem(\"\ud83c\udfb9 Other\"):\n",
    "                other_audio = gr.Audio(label=\"Other Stem\", interactive=False)\n",
    "\n",
    "            with gr.TabItem(\"\ud83d\udcc9 Residual\"):\n",
    "                residual_audio = gr.Audio(\n",
    "                    label=\"Residual (Mix - Stems)\",\n",
    "                    interactive=False,\n",
    "                )\n",
    "                gr.Markdown(\n",
    "                    \"*Residual should be near-silent for good separation. \"\n",
    "                    \"Audible content indicates reconstruction error.*\"\n",
    "                )\n",
    "\n",
    "            with gr.TabItem(\"\ud83d\udd04 Reconstruction\"):\n",
    "                reconstruction_audio = gr.Audio(\n",
    "                    label=\"Reconstructed (Sum of Stems)\",\n",
    "                    interactive=False,\n",
    "                )\n",
    "\n",
    "        # --- Candidate Comparison (if available) ---\n",
    "        gr.Markdown(\"### \ud83c\udfaf Candidate Comparison\")\n",
    "        candidate_table = gr.Markdown(\n",
    "            value=\"Run candidate selection to see comparison.\",\n",
    "        )\n",
    "\n",
    "        # --- Rerun Controls ---\n",
    "        gr.Markdown(\"### \ud83d\udd04 Rerun Controls\")\n",
    "        with gr.Row():\n",
    "            rerun_separation_btn = gr.Button(\n",
    "                \"\ud83c\udf9a\ufe0f Re-run Separation\",\n",
    "                variant=\"secondary\",\n",
    "            )\n",
    "            rerun_transcription_btn = gr.Button(\n",
    "                \"\ud83c\udfb9 Re-run Transcription\",\n",
    "                variant=\"secondary\",\n",
    "            )\n",
    "            refresh_btn = gr.Button(\n",
    "                \"\ud83d\udd04 Refresh Dashboard\",\n",
    "                variant=\"primary\",\n",
    "            )\n",
    "\n",
    "        rerun_output = gr.Markdown(value=\"\")\n",
    "\n",
    "        # --- Load Data Function ---\n",
    "        def refresh_dashboard():\n",
    "            \"\"\"Refresh dashboard with current data.\"\"\"\n",
    "            outputs = {}\n",
    "\n",
    "            # Run ID\n",
    "            outputs[\"run_id\"] = RUN_ID if \"RUN_ID\" in dir() else \"N/A\"\n",
    "            outputs[\"status\"] = \"Ready\" if \"STEM_RESULT\" in dir() else \"No stems available\"\n",
    "\n",
    "            # Stem QA metrics\n",
    "            if \"STEM_QA_RESULT\" in dir() and STEM_QA_RESULT is not None:\n",
    "                outputs[\"stem_qa\"] = format_qa_metrics(STEM_QA_RESULT.metrics)\n",
    "            elif \"STEM_RESULT\" in dir() and STEM_RESULT is not None:\n",
    "                # Compute QA on the fly\n",
    "                try:\n",
    "                    from soundlab.pipeline import QAConfig, score_separation\n",
    "                    import soundfile as sf\n",
    "\n",
    "                    mix_path = SOURCE_PATH if \"SOURCE_PATH\" in dir() else None\n",
    "                    if mix_path and STEM_RESULT.stems:\n",
    "                        mix, sr = sf.read(str(mix_path), dtype=\"float32\")\n",
    "                        stems_data = {}\n",
    "                        for name, path in STEM_RESULT.stems.items():\n",
    "                            stem_audio, _ = sf.read(str(path), dtype=\"float32\")\n",
    "                            stems_data[name] = stem_audio\n",
    "\n",
    "                        qa_result = score_separation(mix, stems_data, sr)\n",
    "                        outputs[\"stem_qa\"] = format_qa_metrics(qa_result.metrics)\n",
    "                        outputs[\"status\"] = \"\u2705 Pass\" if qa_result.passed else \"\u26a0\ufe0f Check QA\"\n",
    "                    else:\n",
    "                        outputs[\"stem_qa\"] = \"Mix path not available for QA.\"\n",
    "                except Exception as e:\n",
    "                    outputs[\"stem_qa\"] = f\"QA computation error: {e}\"\n",
    "            else:\n",
    "                outputs[\"stem_qa\"] = \"Run separation first to see QA metrics.\"\n",
    "\n",
    "            # MIDI QA metrics\n",
    "            if \"MIDI_QA_RESULT\" in dir() and MIDI_QA_RESULT is not None:\n",
    "                outputs[\"midi_qa\"] = format_qa_metrics(MIDI_QA_RESULT.metrics)\n",
    "            else:\n",
    "                outputs[\"midi_qa\"] = \"Run transcription to see MIDI QA metrics.\"\n",
    "\n",
    "            # Audio previews\n",
    "            if \"SOURCE_PATH\" in dir() and SOURCE_PATH:\n",
    "                outputs[\"mix\"] = load_audio_for_preview(SOURCE_PATH)\n",
    "            else:\n",
    "                outputs[\"mix\"] = None\n",
    "\n",
    "            if \"STEM_PATHS\" in dir() and STEM_PATHS:\n",
    "                outputs[\"vocals\"] = load_audio_for_preview(STEM_PATHS.get(\"vocals\"))\n",
    "                outputs[\"drums\"] = load_audio_for_preview(STEM_PATHS.get(\"drums\"))\n",
    "                outputs[\"bass\"] = load_audio_for_preview(STEM_PATHS.get(\"bass\"))\n",
    "                outputs[\"other\"] = load_audio_for_preview(STEM_PATHS.get(\"other\"))\n",
    "\n",
    "                # Compute residual\n",
    "                if \"SOURCE_PATH\" in dir() and SOURCE_PATH:\n",
    "                    outputs[\"residual\"] = compute_residual(SOURCE_PATH, STEM_PATHS)\n",
    "\n",
    "                    # Compute reconstruction\n",
    "                    try:\n",
    "                        import soundfile as sf\n",
    "\n",
    "                        total = None\n",
    "                        sr = None\n",
    "                        for path in STEM_PATHS.values():\n",
    "                            audio, sr = sf.read(str(path), dtype=\"float32\")\n",
    "                            if total is None:\n",
    "                                total = audio\n",
    "                            else:\n",
    "                                total = total + audio\n",
    "                        outputs[\"reconstruction\"] = (sr, total) if total is not None else None\n",
    "                    except Exception:\n",
    "                        outputs[\"reconstruction\"] = None\n",
    "                else:\n",
    "                    outputs[\"residual\"] = None\n",
    "                    outputs[\"reconstruction\"] = None\n",
    "            else:\n",
    "                outputs[\"vocals\"] = None\n",
    "                outputs[\"drums\"] = None\n",
    "                outputs[\"bass\"] = None\n",
    "                outputs[\"other\"] = None\n",
    "                outputs[\"residual\"] = None\n",
    "                outputs[\"reconstruction\"] = None\n",
    "\n",
    "            # Candidate table\n",
    "            if \"candidate_scores\" in dir() and candidate_scores:\n",
    "                candidates = [\n",
    "                    {\"name\": c.name, \"score\": c.score, \"passed\": c.passed} for c in candidate_scores\n",
    "                ]\n",
    "                outputs[\"candidates\"] = format_candidate_table(candidates)\n",
    "            else:\n",
    "                outputs[\"candidates\"] = \"Run candidate selection to see comparison.\"\n",
    "\n",
    "            return (\n",
    "                outputs[\"run_id\"],\n",
    "                outputs[\"status\"],\n",
    "                outputs[\"stem_qa\"],\n",
    "                outputs[\"midi_qa\"],\n",
    "                outputs[\"mix\"],\n",
    "                outputs[\"vocals\"],\n",
    "                outputs[\"drums\"],\n",
    "                outputs[\"bass\"],\n",
    "                outputs[\"other\"],\n",
    "                outputs[\"residual\"],\n",
    "                outputs[\"reconstruction\"],\n",
    "                outputs[\"candidates\"],\n",
    "            )\n",
    "\n",
    "        def handle_rerun_separation():\n",
    "            return \"\u2139\ufe0f To re-run separation, modify settings in the Stem Separation cell and re-execute.\"\n",
    "\n",
    "        def handle_rerun_transcription():\n",
    "            return \"\u2139\ufe0f To re-run transcription, modify settings in the Transcription cell and re-execute.\"\n",
    "\n",
    "        # Wire up refresh button\n",
    "        refresh_btn.click(\n",
    "            fn=refresh_dashboard,\n",
    "            outputs=[\n",
    "                run_id_display,\n",
    "                status_display,\n",
    "                stem_qa_markdown,\n",
    "                midi_qa_markdown,\n",
    "                mix_audio,\n",
    "                vocals_audio,\n",
    "                drums_audio,\n",
    "                bass_audio,\n",
    "                other_audio,\n",
    "                residual_audio,\n",
    "                reconstruction_audio,\n",
    "                candidate_table,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        rerun_separation_btn.click(fn=handle_rerun_separation, outputs=[rerun_output])\n",
    "        rerun_transcription_btn.click(fn=handle_rerun_transcription, outputs=[rerun_output])\n",
    "\n",
    "        # Auto-refresh on load\n",
    "        dashboard.load(\n",
    "            fn=refresh_dashboard,\n",
    "            outputs=[\n",
    "                run_id_display,\n",
    "                status_display,\n",
    "                stem_qa_markdown,\n",
    "                midi_qa_markdown,\n",
    "                mix_audio,\n",
    "                vocals_audio,\n",
    "                drums_audio,\n",
    "                bass_audio,\n",
    "                other_audio,\n",
    "                residual_audio,\n",
    "                reconstruction_audio,\n",
    "                candidate_table,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    return dashboard\n",
    "\n",
    "\n",
    "# Build and launch dashboard\n",
    "print(\"\ud83d\udcca Building QA Dashboard...\")\n",
    "qa_dashboard = build_qa_dashboard()\n",
    "qa_dashboard.launch(height=800, show_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83d\udde3\ufe0f Voice Generation (Optional)\n",
    "# @markdown Generate speech or convert singing voice using extracted stems.\n",
    "# @markdown **Requires:** `install_voice=True` in the installation cell.\n",
    "\n",
    "# --- TTS Settings ---\n",
    "enable_tts = False  # @param {type:\"boolean\"}\n",
    "tts_text = \"Hello, this is a voice generation test.\"  # @param {type:\"string\"}\n",
    "tts_language = \"en\"  # @param [\"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", \"pl\", \"tr\", \"ru\", \"nl\", \"cs\", \"ar\", \"zh-cn\", \"ja\", \"hu\", \"ko\", \"hi\"]\n",
    "tts_speaker_wav = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# --- SVC Settings ---\n",
    "enable_svc = False  # @param {type:\"boolean\"}\n",
    "svc_source_stem = \"vocals\"  # @param [\"vocals\", \"other\"]\n",
    "svc_pitch_shift = 0  # @param {type:\"slider\", min:-12, max:12, step:1}\n",
    "svc_model_path = \"\"  # @param {type:\"string\"}\n",
    "svc_index_path = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# === Execution ===\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if voice module is available\n",
    "VOICE_AVAILABLE = False\n",
    "try:\n",
    "    from soundlab.voice import TTSGenerator, VoiceConverter, TTSConfig, SVCConfig\n",
    "\n",
    "    VOICE_AVAILABLE = True\n",
    "    print(\"\u2705 Voice module available\")\n",
    "except ImportError:\n",
    "    print(\"\u26a0\ufe0f Voice module not installed.\")\n",
    "    print(\"   To enable voice features, set install_voice=True in the installation cell\")\n",
    "    print(\"   and re-run the installation.\")\n",
    "\n",
    "if VOICE_AVAILABLE:\n",
    "    # --- TTS Generation ---\n",
    "    if enable_tts:\n",
    "        print(\"\\n\ud83d\udde3\ufe0f Text-to-Speech Generation\")\n",
    "        print(f\"   Text: {tts_text[:50]}...\")\n",
    "        print(f\"   Language: {tts_language}\")\n",
    "\n",
    "        # Determine speaker reference\n",
    "        speaker_ref = None\n",
    "        if tts_speaker_wav:\n",
    "            speaker_ref = Path(tts_speaker_wav)\n",
    "            if not speaker_ref.exists():\n",
    "                print(f\"   \u26a0\ufe0f Speaker reference not found: {speaker_ref}\")\n",
    "                speaker_ref = None\n",
    "            else:\n",
    "                print(f\"   Speaker ref: {speaker_ref.name}\")\n",
    "        elif \"STEM_PATHS\" in dir() and \"vocals\" in STEM_PATHS:\n",
    "            # Use extracted vocals as speaker reference\n",
    "            speaker_ref = STEM_PATHS[\"vocals\"]\n",
    "            print(f\"   Speaker ref: vocals stem (auto-selected)\")\n",
    "\n",
    "        try:\n",
    "            tts_config = TTSConfig(\n",
    "                language=tts_language,\n",
    "                speaker_wav=speaker_ref,\n",
    "            )\n",
    "\n",
    "            generator = TTSGenerator(config=tts_config)\n",
    "\n",
    "            output_path = VOICE_DIR / \"tts_output.wav\"\n",
    "            print(f\"\\n\ud83d\udd04 Generating speech...\")\n",
    "\n",
    "            result = generator.generate(\n",
    "                text=tts_text,\n",
    "                output_path=output_path,\n",
    "            )\n",
    "\n",
    "            TTS_OUTPUT_PATH = result.output_path\n",
    "            print(f\"\u2705 TTS output saved: {TTS_OUTPUT_PATH}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c TTS generation failed: {e}\")\n",
    "            TTS_OUTPUT_PATH = None\n",
    "    else:\n",
    "        print(\"\\n\u2139\ufe0f TTS generation disabled (set enable_tts=True to enable)\")\n",
    "        TTS_OUTPUT_PATH = None\n",
    "\n",
    "    # --- Singing Voice Conversion ---\n",
    "    if enable_svc:\n",
    "        print(\"\\n\ud83c\udfa4 Singing Voice Conversion\")\n",
    "        print(f\"   Source stem: {svc_source_stem}\")\n",
    "        print(f\"   Pitch shift: {svc_pitch_shift} semitones\")\n",
    "\n",
    "        # Validate source stem\n",
    "        source_path = None\n",
    "        if \"STEM_PATHS\" in dir() and svc_source_stem in STEM_PATHS:\n",
    "            source_path = STEM_PATHS[svc_source_stem]\n",
    "            print(f\"   Source: {source_path.name}\")\n",
    "        else:\n",
    "            print(f\"   \u26a0\ufe0f Source stem '{svc_source_stem}' not found\")\n",
    "            print(\"      Run stem separation first\")\n",
    "\n",
    "        # Validate model path\n",
    "        model_path = Path(svc_model_path) if svc_model_path else None\n",
    "        if model_path and not model_path.exists():\n",
    "            print(f\"   \u26a0\ufe0f Model not found: {model_path}\")\n",
    "            print(\"      RVC models require manual setup. See documentation.\")\n",
    "            model_path = None\n",
    "\n",
    "        if source_path and model_path:\n",
    "            try:\n",
    "                svc_config = SVCConfig(\n",
    "                    model_path=model_path,\n",
    "                    index_path=Path(svc_index_path) if svc_index_path else None,\n",
    "                    pitch_shift_semitones=svc_pitch_shift,\n",
    "                )\n",
    "\n",
    "                converter = VoiceConverter(config=svc_config)\n",
    "\n",
    "                output_path = VOICE_DIR / f\"svc_{svc_source_stem}.wav\"\n",
    "                print(f\"\\n\ud83d\udd04 Converting voice...\")\n",
    "\n",
    "                result = converter.convert(\n",
    "                    audio_path=source_path,\n",
    "                    output_path=output_path,\n",
    "                )\n",
    "\n",
    "                SVC_OUTPUT_PATH = result.output_path\n",
    "                print(f\"\u2705 SVC output saved: {SVC_OUTPUT_PATH}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\u274c Voice conversion failed: {e}\")\n",
    "                SVC_OUTPUT_PATH = None\n",
    "        else:\n",
    "            if not model_path:\n",
    "                print(\"\\n\ud83d\udca1 RVC Setup Instructions:\")\n",
    "                print(\"   1. Download a compatible RVC model (.pth file)\")\n",
    "                print(\"   2. Optionally download the index file (.index)\")\n",
    "                print(\"   3. Upload to Colab or mount from Drive\")\n",
    "                print(\"   4. Set svc_model_path and svc_index_path above\")\n",
    "            SVC_OUTPUT_PATH = None\n",
    "    else:\n",
    "        print(\"\\n\u2139\ufe0f SVC disabled (set enable_svc=True to enable)\")\n",
    "        SVC_OUTPUT_PATH = None\n",
    "\n",
    "    # --- Summary ---\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"\ud83d\udde3\ufe0f Voice Generation Summary\")\n",
    "    print(\"=\" * 50)\n",
    "    if TTS_OUTPUT_PATH:\n",
    "        print(f\"   TTS Output: {TTS_OUTPUT_PATH}\")\n",
    "    if SVC_OUTPUT_PATH:\n",
    "        print(f\"   SVC Output: {SVC_OUTPUT_PATH}\")\n",
    "    if not TTS_OUTPUT_PATH and not SVC_OUTPUT_PATH:\n",
    "        print(\"   No voice outputs generated\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\ud83d\udca1 To enable voice features:\")\n",
    "    print(\"   1. Go to the 'Install SoundLab' cell\")\n",
    "    print(\"   2. Set install_voice = True\")\n",
    "    print(\"   3. Re-run the installation cell\")\n",
    "    print(\"   4. Return here and configure voice settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83d\udcbe Export & Download\n",
    "# @markdown Export all processed files and download as a ZIP archive.\n",
    "\n",
    "# --- Export Settings ---\n",
    "export_stems = True  # @param {type:\"boolean\"}\n",
    "export_midi = True  # @param {type:\"boolean\"}\n",
    "export_voice = True  # @param {type:\"boolean\"}\n",
    "export_analysis = True  # @param {type:\"boolean\"}\n",
    "export_config = True  # @param {type:\"boolean\"}\n",
    "export_qa_report = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# --- Format Options ---\n",
    "stem_format = \"wav\"  # @param [\"wav\", \"mp3\", \"flac\"]\n",
    "normalize_stems = True  # @param {type:\"boolean\"}\n",
    "normalization_lufs = -14.0  # @param {type:\"number\"}\n",
    "\n",
    "# --- Archive Options ---\n",
    "create_zip_archive = True  # @param {type:\"boolean\"}\n",
    "zip_filename = \"soundlab_export\"  # @param {type:\"string\"}\n",
    "auto_download = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# === Execution ===\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Collect files to export\n",
    "export_files: list[Path] = []\n",
    "export_manifest: dict[str, list[str]] = {\n",
    "    \"stems\": [],\n",
    "    \"midi\": [],\n",
    "    \"voice\": [],\n",
    "    \"analysis\": [],\n",
    "    \"reports\": [],\n",
    "}\n",
    "\n",
    "print(\"\ud83d\udce6 Collecting files for export...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# --- Stems ---\n",
    "if export_stems and \"STEM_PATHS\" in dir() and STEM_PATHS:\n",
    "    print(\"\\n\ud83c\udfbc Stems:\")\n",
    "\n",
    "    if normalize_stems and normalization_lufs:\n",
    "        try:\n",
    "            from soundlab.io import load_audio\n",
    "            from soundlab.io.export import export_audio\n",
    "\n",
    "            # Re-export stems with normalization\n",
    "            normalized_dir = EXPORTS_DIR / \"stems\"\n",
    "            normalized_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            for name, path in STEM_PATHS.items():\n",
    "                audio = load_audio(path)\n",
    "                export_path = normalized_dir / f\"{name}.{stem_format}\"\n",
    "                export_audio(\n",
    "                    audio,\n",
    "                    export_path,\n",
    "                    format=stem_format,\n",
    "                    normalize_lufs=normalization_lufs,\n",
    "                )\n",
    "                export_files.append(export_path)\n",
    "                export_manifest[\"stems\"].append(str(export_path.name))\n",
    "                print(f\"   \u2705 {name}: {export_path.name} (normalized to {normalization_lufs} LUFS)\")\n",
    "        except ImportError as e:\n",
    "            print(f\"   \u26a0\ufe0f Normalization requires pyloudnorm: {e}\")\n",
    "            # Fall back to raw stems\n",
    "            for name, path in STEM_PATHS.items():\n",
    "                export_files.append(Path(path))\n",
    "                export_manifest[\"stems\"].append(Path(path).name)\n",
    "                print(f\"   \u2705 {name}: {Path(path).name}\")\n",
    "    else:\n",
    "        for name, path in STEM_PATHS.items():\n",
    "            export_files.append(Path(path))\n",
    "            export_manifest[\"stems\"].append(Path(path).name)\n",
    "            print(f\"   \u2705 {name}: {Path(path).name}\")\n",
    "else:\n",
    "    print(\"\\n\ud83c\udfbc Stems: (none available)\")\n",
    "\n",
    "# --- MIDI ---\n",
    "if export_midi and \"MIDI_PATHS\" in dir() and MIDI_PATHS:\n",
    "    print(\"\\n\ud83c\udfb9 MIDI:\")\n",
    "    for name, path in MIDI_PATHS.items():\n",
    "        export_files.append(Path(path))\n",
    "        export_manifest[\"midi\"].append(Path(path).name)\n",
    "        print(f\"   \u2705 {name}: {Path(path).name}\")\n",
    "elif export_midi:\n",
    "    # Check MIDI directory for any files\n",
    "    if MIDI_DIR.exists():\n",
    "        midi_files = list(MIDI_DIR.glob(\"*.mid\")) + list(MIDI_DIR.glob(\"*.midi\"))\n",
    "        if midi_files:\n",
    "            print(\"\\n\ud83c\udfb9 MIDI:\")\n",
    "            for path in midi_files:\n",
    "                export_files.append(path)\n",
    "                export_manifest[\"midi\"].append(path.name)\n",
    "                print(f\"   \u2705 {path.name}\")\n",
    "        else:\n",
    "            print(\"\\n\ud83c\udfb9 MIDI: (none available)\")\n",
    "    else:\n",
    "        print(\"\\n\ud83c\udfb9 MIDI: (none available)\")\n",
    "\n",
    "# --- Voice ---\n",
    "if export_voice:\n",
    "    voice_files = []\n",
    "    if \"TTS_OUTPUT_PATH\" in dir() and TTS_OUTPUT_PATH:\n",
    "        voice_files.append(Path(TTS_OUTPUT_PATH))\n",
    "    if \"SVC_OUTPUT_PATH\" in dir() and SVC_OUTPUT_PATH:\n",
    "        voice_files.append(Path(SVC_OUTPUT_PATH))\n",
    "    if VOICE_DIR.exists():\n",
    "        voice_files.extend(VOICE_DIR.glob(\"*.wav\"))\n",
    "\n",
    "    if voice_files:\n",
    "        print(\"\\n\ud83d\udde3\ufe0f Voice:\")\n",
    "        for path in voice_files:\n",
    "            if path.exists():\n",
    "                export_files.append(path)\n",
    "                export_manifest[\"voice\"].append(path.name)\n",
    "                print(f\"   \u2705 {path.name}\")\n",
    "    else:\n",
    "        print(\"\\n\ud83d\udde3\ufe0f Voice: (none available)\")\n",
    "\n",
    "# --- Analysis ---\n",
    "if export_analysis:\n",
    "    analysis_files = []\n",
    "    if ANALYSIS_DIR.exists():\n",
    "        analysis_files.extend(ANALYSIS_DIR.glob(\"*\"))\n",
    "\n",
    "    if analysis_files:\n",
    "        print(\"\\n\ud83d\udcca Analysis:\")\n",
    "        for path in analysis_files:\n",
    "            if path.is_file():\n",
    "                export_files.append(path)\n",
    "                export_manifest[\"analysis\"].append(path.name)\n",
    "                print(f\"   \u2705 {path.name}\")\n",
    "    else:\n",
    "        print(\"\\n\ud83d\udcca Analysis: (none available)\")\n",
    "\n",
    "# --- Config Export ---\n",
    "if export_config:\n",
    "    print(\"\\n\u2699\ufe0f Configuration:\")\n",
    "    config_path = EXPORTS_DIR / \"config.json\"\n",
    "\n",
    "    config_data = {\n",
    "        \"run_id\": RUN_ID if \"RUN_ID\" in dir() else None,\n",
    "        \"audio_hash\": AUDIO_HASH if \"AUDIO_HASH\" in dir() else None,\n",
    "        \"source_file\": str(SOURCE_PATH.name) if \"SOURCE_PATH\" in dir() and SOURCE_PATH else None,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"separation\": {},\n",
    "        \"transcription\": {},\n",
    "        \"voice\": {},\n",
    "        \"environment\": {\n",
    "            \"gpu_available\": GPU_AVAILABLE if \"GPU_AVAILABLE\" in dir() else False,\n",
    "            \"gpu_name\": GPU_NAME if \"GPU_NAME\" in dir() else None,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Add separation config if available\n",
    "    if \"STEM_RESULT\" in dir() and STEM_RESULT:\n",
    "        config_data[\"separation\"] = {\n",
    "            \"model\": str(STEM_RESULT.config.model.value),\n",
    "            \"segment_length\": STEM_RESULT.config.segment_length,\n",
    "            \"overlap\": STEM_RESULT.config.overlap,\n",
    "            \"shifts\": STEM_RESULT.config.shifts,\n",
    "            \"processing_time_seconds\": STEM_RESULT.processing_time_seconds,\n",
    "        }\n",
    "\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config_data, f, indent=2, default=str)\n",
    "\n",
    "    export_files.append(config_path)\n",
    "    export_manifest[\"reports\"].append(config_path.name)\n",
    "    print(f\"   \u2705 {config_path.name}\")\n",
    "\n",
    "# --- QA Report ---\n",
    "if export_qa_report:\n",
    "    print(\"\\n\ud83d\udccb QA Report:\")\n",
    "    qa_report_path = EXPORTS_DIR / \"qa_report.csv\"\n",
    "\n",
    "    qa_rows = [[\"Metric\", \"Value\", \"Threshold\", \"Status\"]]\n",
    "\n",
    "    # Add stem QA metrics\n",
    "    if \"STEM_QA_RESULT\" in dir() and STEM_QA_RESULT:\n",
    "        for metric, value in STEM_QA_RESULT.metrics.items():\n",
    "            qa_rows.append([metric, f\"{value:.4f}\", \"\", \"\"])\n",
    "        qa_rows.append(\n",
    "            [\n",
    "                \"Overall Score\",\n",
    "                f\"{STEM_QA_RESULT.score:.4f}\",\n",
    "                \"0.70\",\n",
    "                \"PASS\" if STEM_QA_RESULT.passed else \"FAIL\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Add MIDI QA metrics\n",
    "    if \"MIDI_QA_RESULT\" in dir() and MIDI_QA_RESULT:\n",
    "        qa_rows.append([\"\", \"\", \"\", \"\"])  # Separator\n",
    "        qa_rows.append([\"MIDI QA\", \"\", \"\", \"\"])\n",
    "        for metric, value in MIDI_QA_RESULT.metrics.items():\n",
    "            qa_rows.append([metric, f\"{value:.4f}\", \"\", \"\"])\n",
    "        qa_rows.append(\n",
    "            [\n",
    "                \"MIDI Score\",\n",
    "                f\"{MIDI_QA_RESULT.score:.4f}\",\n",
    "                \"0.60\",\n",
    "                \"PASS\" if MIDI_QA_RESULT.passed else \"FAIL\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if len(qa_rows) > 1:\n",
    "        with open(qa_report_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(qa_rows)\n",
    "\n",
    "        export_files.append(qa_report_path)\n",
    "        export_manifest[\"reports\"].append(qa_report_path.name)\n",
    "        print(f\"   \u2705 {qa_report_path.name}\")\n",
    "    else:\n",
    "        print(\"   \u2139\ufe0f No QA data available\")\n",
    "\n",
    "# --- Create ZIP Archive ---\n",
    "if create_zip_archive and export_files:\n",
    "    print(\"\\n\ud83d\udce6 Creating ZIP archive...\")\n",
    "\n",
    "    # Generate timestamped filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_suffix = f\"_{RUN_ID[:8]}\" if \"RUN_ID\" in dir() and RUN_ID else \"\"\n",
    "    zip_path = EXPORTS_DIR / f\"{zip_filename}{run_suffix}_{timestamp}.zip\"\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        for file_path in export_files:\n",
    "            if file_path.exists():\n",
    "                # Determine subdirectory based on file type\n",
    "                if (\n",
    "                    file_path.suffix in [\".wav\", \".mp3\", \".flac\"]\n",
    "                    and \"stem\" in str(file_path.parent).lower()\n",
    "                ):\n",
    "                    arcname = f\"stems/{file_path.name}\"\n",
    "                elif file_path.suffix in [\".mid\", \".midi\"]:\n",
    "                    arcname = f\"midi/{file_path.name}\"\n",
    "                elif (\n",
    "                    file_path.parent.name == \"voice\"\n",
    "                    or \"tts\" in file_path.stem\n",
    "                    or \"svc\" in file_path.stem\n",
    "                ):\n",
    "                    arcname = f\"voice/{file_path.name}\"\n",
    "                elif file_path.suffix == \".json\" or file_path.suffix == \".csv\":\n",
    "                    arcname = f\"reports/{file_path.name}\"\n",
    "                else:\n",
    "                    arcname = file_path.name\n",
    "\n",
    "                zf.write(file_path, arcname=arcname)\n",
    "\n",
    "        # Add manifest\n",
    "        manifest_json = json.dumps(export_manifest, indent=2)\n",
    "        zf.writestr(\"manifest.json\", manifest_json)\n",
    "\n",
    "    EXPORT_ZIP_PATH = zip_path\n",
    "    zip_size_mb = zip_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"   \u2705 {zip_path.name} ({zip_size_mb:.1f} MB)\")\n",
    "\n",
    "    # Auto-download in Colab\n",
    "    if auto_download:\n",
    "        try:\n",
    "            from google.colab import files as colab_files\n",
    "\n",
    "            print(\"\\n\ud83d\udce5 Initiating download...\")\n",
    "            colab_files.download(str(zip_path))\n",
    "            print(\"\u2705 Download started!\")\n",
    "        except ImportError:\n",
    "            print(\"\\n\u2139\ufe0f Not running in Colab. Download manually from:\")\n",
    "            print(f\"   {zip_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f Auto-download failed: {e}\")\n",
    "            print(f\"   Manual download path: {zip_path}\")\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\ud83d\udce6 Export Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"   Total files: {len(export_files)}\")\n",
    "print(f\"   Stems: {len(export_manifest['stems'])}\")\n",
    "print(f\"   MIDI: {len(export_manifest['midi'])}\")\n",
    "print(f\"   Voice: {len(export_manifest['voice'])}\")\n",
    "print(f\"   Analysis: {len(export_manifest['analysis'])}\")\n",
    "print(f\"   Reports: {len(export_manifest['reports'])}\")\n",
    "if \"EXPORT_ZIP_PATH\" in dir():\n",
    "    print(f\"\\n   \ud83d\udcc1 Archive: {EXPORT_ZIP_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83e\uddf9 Cleanup\n",
    "# @markdown Clean up temporary files and free GPU memory.\n",
    "\n",
    "# --- Cleanup Options ---\n",
    "cleanup_stems = False  # @param {type:\"boolean\"}\n",
    "cleanup_midi = False  # @param {type:\"boolean\"}\n",
    "cleanup_voice = False  # @param {type:\"boolean\"}\n",
    "cleanup_analysis = False  # @param {type:\"boolean\"}\n",
    "cleanup_exports = False  # @param {type:\"boolean\"}\n",
    "cleanup_cache = False  # @param {type:\"boolean\"}\n",
    "clear_gpu_cache = True  # @param {type:\"boolean\"}\n",
    "reset_global_state = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# === Execution ===\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def safe_rmtree(path: Path, name: str) -> int:\n",
    "    \"\"\"Safely remove a directory and return count of removed files.\"\"\"\n",
    "    if not path.exists():\n",
    "        return 0\n",
    "\n",
    "    count = sum(1 for _ in path.rglob(\"*\") if _.is_file())\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        path.mkdir(exist_ok=True)  # Recreate empty directory\n",
    "        print(f\"   \u2705 {name}: Removed {count} files\")\n",
    "        return count\n",
    "    except Exception as e:\n",
    "        print(f\"   \u26a0\ufe0f {name}: Failed to clean - {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "print(\"\ud83e\uddf9 Cleanup\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_removed = 0\n",
    "\n",
    "# --- Directory Cleanup ---\n",
    "if cleanup_stems and \"STEMS_DIR\" in dir():\n",
    "    total_removed += safe_rmtree(STEMS_DIR, \"Stems\")\n",
    "\n",
    "if cleanup_midi and \"MIDI_DIR\" in dir():\n",
    "    total_removed += safe_rmtree(MIDI_DIR, \"MIDI\")\n",
    "\n",
    "if cleanup_voice and \"VOICE_DIR\" in dir():\n",
    "    total_removed += safe_rmtree(VOICE_DIR, \"Voice\")\n",
    "\n",
    "if cleanup_analysis and \"ANALYSIS_DIR\" in dir():\n",
    "    total_removed += safe_rmtree(ANALYSIS_DIR, \"Analysis\")\n",
    "\n",
    "if cleanup_exports and \"EXPORTS_DIR\" in dir():\n",
    "    total_removed += safe_rmtree(EXPORTS_DIR, \"Exports\")\n",
    "\n",
    "if cleanup_cache and \"CACHE_ROOT\" in dir():\n",
    "    # Clean checkpoints and cache, preserve models\n",
    "    if CHECKPOINTS_DIR.exists():\n",
    "        total_removed += safe_rmtree(CHECKPOINTS_DIR, \"Checkpoints\")\n",
    "\n",
    "    cache_dir = CACHE_ROOT / \"cache\"\n",
    "    if cache_dir.exists():\n",
    "        total_removed += safe_rmtree(cache_dir, \"Cache\")\n",
    "\n",
    "# --- GPU Memory ---\n",
    "if clear_gpu_cache:\n",
    "    print(\"\\n\ud83c\udfae GPU Memory:\")\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            before = torch.cuda.memory_allocated() / 1e9\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            after = torch.cuda.memory_allocated() / 1e9\n",
    "            freed = before - after\n",
    "            print(f\"   \u2705 Cleared GPU cache: {freed:.2f} GB freed\")\n",
    "            print(f\"   Current allocation: {after:.2f} GB\")\n",
    "        else:\n",
    "            print(\"   \u2139\ufe0f No GPU available\")\n",
    "    except ImportError:\n",
    "        print(\"   \u2139\ufe0f PyTorch not available\")\n",
    "\n",
    "# --- Python Garbage Collection ---\n",
    "print(\"\\n\ud83d\udc0d Python Memory:\")\n",
    "gc.collect()\n",
    "print(\"   \u2705 Garbage collection complete\")\n",
    "\n",
    "# --- Reset Global State ---\n",
    "if reset_global_state:\n",
    "    print(\"\\n\ud83d\udd04 Resetting Global State:\")\n",
    "\n",
    "    # List of global variables to reset\n",
    "    globals_to_reset = [\n",
    "        \"CURRENT_AUDIO\",\n",
    "        \"CANONICAL_AUDIO\",\n",
    "        \"EXCERPT_AUDIO\",\n",
    "        \"AUDIO_HASH\",\n",
    "        \"SOURCE_PATH\",\n",
    "        \"RUN_ID\",\n",
    "        \"STEM_RESULT\",\n",
    "        \"STEM_PATHS\",\n",
    "        \"STEM_QA_RESULT\",\n",
    "        \"MIDI_PATHS\",\n",
    "        \"MIDI_QA_RESULT\",\n",
    "        \"TTS_OUTPUT_PATH\",\n",
    "        \"SVC_OUTPUT_PATH\",\n",
    "        \"EXPORT_ZIP_PATH\",\n",
    "        \"BEST_CANDIDATE\",\n",
    "        \"BEST_PLAN\",\n",
    "        \"candidate_scores\",\n",
    "    ]\n",
    "\n",
    "    reset_count = 0\n",
    "    for var_name in globals_to_reset:\n",
    "        if var_name in globals():\n",
    "            globals()[var_name] = None\n",
    "            reset_count += 1\n",
    "\n",
    "    print(f\"   \u2705 Reset {reset_count} global variables\")\n",
    "    print(\"   \u2139\ufe0f Re-run the Upload cell to start a new session\")\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\ud83e\uddf9 Cleanup Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"   Files removed: {total_removed}\")\n",
    "print(f\"   GPU cache cleared: {'Yes' if clear_gpu_cache else 'No'}\")\n",
    "print(f\"   Global state reset: {'Yes' if reset_global_state else 'No'}\")\n",
    "\n",
    "if not any(\n",
    "    [\n",
    "        cleanup_stems,\n",
    "        cleanup_midi,\n",
    "        cleanup_voice,\n",
    "        cleanup_analysis,\n",
    "        cleanup_exports,\n",
    "        cleanup_cache,\n",
    "    ]\n",
    "):\n",
    "    print(\"\\n\ud83d\udca1 Tip: Enable cleanup options above to remove temporary files\")\n",
    "    print(\"   before downloading to free up disk space.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}