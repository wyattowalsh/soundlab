{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéπ MIDI Transcription with SoundLab\n",
    "\n",
    "This notebook demonstrates how to transcribe audio to MIDI using Basic Pitch, then visualize and export the results.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Loading piano or instrument recordings\n",
    "- Configuring the transcription engine\n",
    "- Generating MIDI from audio\n",
    "- Visualizing with piano roll\n",
    "- Exporting MIDI files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install SoundLab and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SoundLab (uncomment if running in Colab)\n",
    "# !pip install soundlab[transcription]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from soundlab.io import load_audio\n",
    "from soundlab.io.midi_io import save_midi\n",
    "from soundlab.transcription import BasicPitchTranscriber, TranscriptionConfig\n",
    "from soundlab.transcription.viz import plot_piano_roll\n",
    "\n",
    "print(\"‚úÖ SoundLab imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Your Audio\n",
    "\n",
    "Load a piano or instrument recording. For best results, use:\n",
    "- Clean recordings without background noise\n",
    "- Monophonic or simple polyphonic content\n",
    "- Isolated stems (use stem separation first for mixed tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Upload or specify audio path\n",
    "# @markdown Provide a path to a piano or instrument recording.\n",
    "\n",
    "AUDIO_PATH = \"piano_recording.wav\"  # @param {type: \"string\"}\n",
    "\n",
    "# For Colab: uncomment to upload\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# AUDIO_PATH = list(uploaded.keys())[0]\n",
    "\n",
    "# Load the audio\n",
    "audio = load_audio(AUDIO_PATH)\n",
    "\n",
    "print(f\"üìÅ Loaded: {AUDIO_PATH}\")\n",
    "print(f\"   Duration: {audio.duration:.2f}s\")\n",
    "print(f\"   Sample rate: {audio.sample_rate} Hz\")\n",
    "print(f\"   Channels: {audio.channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the audio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "print(\"üéß Original Audio:\")\n",
    "display(Audio(audio.samples.T, rate=audio.sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Transcription\n",
    "\n",
    "Basic Pitch uses neural networks to detect pitch and notes. You can tune several parameters:\n",
    "\n",
    "| Parameter | Description | Range |\n",
    "|-----------|-------------|-------|\n",
    "| `onset_threshold` | Note start sensitivity | 0.1 - 0.9 |\n",
    "| `frame_threshold` | Note continuation sensitivity | 0.1 - 0.9 |\n",
    "| `min_note_length` | Minimum note duration (seconds) | 0.01 - 0.5 |\n",
    "| `min_freq` | Minimum frequency to detect (Hz) | 20 - 500 |\n",
    "| `max_freq` | Maximum frequency to detect (Hz) | 1000 - 8000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Transcription Configuration\n",
    "# @markdown Tune the transcription parameters for your audio.\n",
    "\n",
    "ONSET_THRESHOLD = 0.5  # @param {type: \"slider\", min: 0.1, max: 0.9, step: 0.05}\n",
    "FRAME_THRESHOLD = 0.3  # @param {type: \"slider\", min: 0.1, max: 0.9, step: 0.05}\n",
    "MIN_NOTE_LENGTH = 0.058  # @param {type: \"slider\", min: 0.01, max: 0.5, step: 0.01}\n",
    "MIN_FREQ = 32  # @param {type: \"slider\", min: 20, max: 500, step: 10}\n",
    "MAX_FREQ = 2000  # @param {type: \"slider\", min: 1000, max: 8000, step: 100}\n",
    "\n",
    "# Create configuration\n",
    "config = TranscriptionConfig(\n",
    "    onset_threshold=ONSET_THRESHOLD,\n",
    "    frame_threshold=FRAME_THRESHOLD,\n",
    "    min_note_length=MIN_NOTE_LENGTH,\n",
    "    min_freq=MIN_FREQ,\n",
    "    max_freq=MAX_FREQ,\n",
    ")\n",
    "\n",
    "print(\"üéõÔ∏è Configuration:\")\n",
    "print(f\"   Onset threshold: {config.onset_threshold}\")\n",
    "print(f\"   Frame threshold: {config.frame_threshold}\")\n",
    "print(f\"   Min note length: {config.min_note_length}s\")\n",
    "print(f\"   Frequency range: {config.min_freq} - {config.max_freq} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Transcription\n",
    "\n",
    "Now let's transcribe the audio to MIDI using Basic Pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transcriber\n",
    "transcriber = BasicPitchTranscriber(config)\n",
    "\n",
    "# Run transcription\n",
    "print(\"üéπ Transcribing audio to MIDI...\")\n",
    "result = transcriber.transcribe(audio)\n",
    "\n",
    "print(\"\\n‚úÖ Transcription complete!\")\n",
    "print(f\"   Notes detected: {len(result.notes)}\")\n",
    "print(f\"   Duration: {result.duration:.2f}s\")\n",
    "print(f\"   Processing time: {result.processing_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore the Results\n",
    "\n",
    "Let's look at the detected notes and their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 10 notes\n",
    "print(\"üìã First 10 notes:\")\n",
    "print(f\"{'#':<4} {'Pitch':<6} {'Start':<8} {'Duration':<10} {'Velocity':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for i, note in enumerate(result.notes[:10]):\n",
    "    print(\n",
    "        f\"{i + 1:<4} {note.pitch:<6} {note.start_time:<8.3f} {note.duration:<10.3f} {note.velocity:<10}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "pitches = [n.pitch for n in result.notes]\n",
    "durations = [n.duration for n in result.notes]\n",
    "velocities = [n.velocity for n in result.notes]\n",
    "\n",
    "print(\"üìä Note Statistics:\")\n",
    "print(f\"   Pitch range: {min(pitches)} - {max(pitches)} (MIDI)\")\n",
    "print(f\"   Duration range: {min(durations):.3f} - {max(durations):.3f}s\")\n",
    "print(f\"   Avg velocity: {np.mean(velocities):.1f}\")\n",
    "print(f\"   Total note time: {sum(durations):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize with Piano Roll\n",
    "\n",
    "The piano roll visualization shows pitch on the Y-axis and time on the X-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot piano roll\n",
    "fig = plot_piano_roll(\n",
    "    result.notes,\n",
    "    title=\"Transcribed Piano Roll\",\n",
    "    figsize=(14, 6),\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Zoom into a specific time range\n",
    "# @markdown Adjust to focus on a specific section.\n",
    "\n",
    "START_TIME = 0.0  # @param {type: \"number\"}\n",
    "END_TIME = 10.0  # @param {type: \"number\"}\n",
    "\n",
    "# Filter notes in time range\n",
    "notes_in_range = [n for n in result.notes if START_TIME <= n.start_time <= END_TIME]\n",
    "\n",
    "fig = plot_piano_roll(\n",
    "    notes_in_range,\n",
    "    title=f\"Piano Roll ({START_TIME:.1f}s - {END_TIME:.1f}s)\",\n",
    "    figsize=(14, 6),\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üéµ Notes in range: {len(notes_in_range)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export MIDI\n",
    "\n",
    "Save the transcription as a standard MIDI file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Export Settings\n",
    "# @markdown Configure the MIDI export.\n",
    "\n",
    "OUTPUT_PATH = \"transcribed.mid\"  # @param {type: \"string\"}\n",
    "TEMPO = 120  # @param {type: \"integer\"}\n",
    "\n",
    "# Save MIDI\n",
    "save_midi(result, OUTPUT_PATH, tempo=TEMPO)\n",
    "\n",
    "print(f\"üíæ Saved MIDI to: {OUTPUT_PATH}\")\n",
    "print(f\"   Tempo: {TEMPO} BPM\")\n",
    "print(f\"   Notes: {len(result.notes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab: Download the MIDI file\n",
    "# from google.colab import files\n",
    "# files.download(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Parameters\n",
    "\n",
    "Try different threshold values to see how they affect transcription quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Parameter Comparison\n",
    "# @markdown Compare transcription with different settings.\n",
    "\n",
    "COMPARE_ONSET = [0.3, 0.5, 0.7]  # Different onset thresholds\n",
    "\n",
    "results = {}\n",
    "for onset in COMPARE_ONSET:\n",
    "    cfg = TranscriptionConfig(onset_threshold=onset)\n",
    "    transcriber = BasicPitchTranscriber(cfg)\n",
    "    res = transcriber.transcribe(audio)\n",
    "    results[onset] = res\n",
    "    print(f\"Onset={onset}: {len(res.notes)} notes detected\")\n",
    "\n",
    "print(\"\\nüìä Lower threshold = more notes (possibly more false positives)\")\n",
    "print(\"üìä Higher threshold = fewer notes (possibly missing soft notes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(len(COMPARE_ONSET), 1, figsize=(14, 4 * len(COMPARE_ONSET)))\n",
    "\n",
    "for ax, onset in zip(axes, COMPARE_ONSET, strict=True):\n",
    "    notes = results[onset].notes\n",
    "    for note in notes:\n",
    "        ax.barh(\n",
    "            note.pitch,\n",
    "            note.duration,\n",
    "            left=note.start_time,\n",
    "            height=0.8,\n",
    "            alpha=note.velocity / 127,\n",
    "            color=\"steelblue\",\n",
    "        )\n",
    "    ax.set_ylabel(\"MIDI Pitch\")\n",
    "    ax.set_title(f\"Onset Threshold = {onset} ({len(notes)} notes)\")\n",
    "\n",
    "axes[-1].set_xlabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Done!\n",
    "\n",
    "You've successfully transcribed audio to MIDI using SoundLab.\n",
    "\n",
    "**Next steps:**\n",
    "- Import the MIDI file into your DAW\n",
    "- Try the [Stem Separation](./stem_separation.ipynb) notebook to isolate instruments first\n",
    "- Explore the [Voice Conversion](./voice_conversion.ipynb) notebook for TTS\n",
    "- Check out the [SoundLab Studio](../soundlab_studio.ipynb) for the full pipeline\n",
    "\n",
    "**Tips:**\n",
    "- For polyphonic audio, separate stems first then transcribe each\n",
    "- Lower thresholds for soft/quiet recordings\n",
    "- Higher thresholds to reduce noise in noisy recordings\n",
    "- Use `min_note_length` to filter out very short spurious notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
